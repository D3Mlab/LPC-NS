{"time_milp": 711.2923822402954, "time_greedy": 0.4943838596343994, "time_refit_milp_assignment": 714.2759182453156, "mse_refit_ground_truth_assignment": 0.7673352590041322, "r2_refit_ground_truth_assignment": 0.9935305894203541, "weight_mismatch_refit_ground_truth_assignment": 1.583458665238845, "refit-weight_mismatch_refit_ground_truth_assignment": 0.0, "rand_score_refit_ground_truth_assignment": 1.0, "label_mismatch_refit_ground_truth_assignment": 0.0, "mse_milp": 0.7330049745100182, "r2_milp": 0.9938200283625926, "weight_mismatch_milp": 1.4649874214908096, "refit-weight_mismatch_milp": 0.16842958341157138, "rand_score_milp": 0.9816118935837246, "label_mismatch_milp": 0.013888888888888888, "mse_refit_milp_assignment": 0.7329623254446944, "r2_refit_milp_assignment": 0.993820387937252, "weight_mismatch_refit_milp_assignment": 1.4653033596821665, "refit-weight_mismatch_refit_milp_assignment": 0.1357511856661929, "rand_score_refit_milp_assignment": 0.9816118935837246, "label_mismatch_refit_milp_assignment": 0.013888888888888888, "mse_greedy": 6.599741475381951, "r2_greedy": 0.9443575193205949, "weight_mismatch_greedy": 12.789087926017435, "refit-weight_mismatch_greedy": 12.361826470777423, "rand_score_greedy": 0.7889866979655713, "label_mismatch_greedy": 0.2652777777777778, "mse_greedy_sem": 1.8001015597568146, "r2_greedy_sem": 0.015176672697461872, "weight_mismatch_greedy_sem": 2.827554664274539, "refit-weight_mismatch_greedy_sem": 2.9295318663949637, "rand_score_greedy_sem": 0.03055644372736865, "label_mismatch_greedy_sem": 0.044573323738350726, "mse_ground_truth": 0.791489333529926, "r2_ground_truth": 0.9929623779880872, "weight_mismatch_ground_truth": 0, "rand_score_ground_truth": 1, "label_mismatch_ground_truth": 0, "mse_baseline_sklearn": 110.80486182514272, "r2_baseline_sklearn": 0.06580319755133035, "mse_milp_val": 0.7875392489085904, "r2_milp_val": 0.9923576670330546, "label_mismatch_milp_val": 0.0, "mse_refit_milp_assignment_val": 0.7906248481672016, "r2_refit_milp_assignment_val": 0.9923277241737374, "label_mismatch_refit_milp_assignment_val": 0.0, "mse_greedy_val": 17.357134260273348, "label_mismatch_greedy_val": 0.2416666666666667, "mse_greedy_val_sem": 5.195088286585498, "label_mismatch_greedy_val_sem": 0.04525149023425695, "r2_greedy_val": 0.8315652209679503, "r2_greedy_val_sem": 0.050413480386896456, "mse_refit_ground_truth_assignment_val": 0.8156368141997485, "r2_refit_ground_truth_assignment_val": 0.9920850064008219, "label_mismatch_refit_ground_truth_assignment_val": 0.0, "mse_ground_truth_val": 0.6059006358510087, "r2_ground_truth_val": 0.9941203001495169, "label_mismatch_ground_truth_val": 0.0, "mse_baseline_sklearn_val": 103.39386765163992, "r2_baseline_sklearn_val": -0.0033409311715904}
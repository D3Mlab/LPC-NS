{"time_milp": 1392.546442747116, "time_greedy": 0.5362796187400818, "time_refit_milp_assignment": 1395.6144115924835, "mse_refit_ground_truth_assignment": 1.1462662511049386, "r2_refit_ground_truth_assignment": 0.9903873010999502, "weight_mismatch_refit_ground_truth_assignment": 1.935338368625642, "refit-weight_mismatch_refit_ground_truth_assignment": 0.0, "rand_score_refit_ground_truth_assignment": 1.0, "label_mismatch_refit_ground_truth_assignment": 0.0, "mse_milp": 1.0862069860332282, "r2_milp": 0.990890963866551, "weight_mismatch_milp": 1.5270253807127276, "refit-weight_mismatch_milp": 0.4997024722983386, "rand_score_milp": 0.9471830985915493, "label_mismatch_milp": 0.041666666666666664, "mse_refit_milp_assignment": 1.0861622492169292, "r2_refit_milp_assignment": 0.9908913390337902, "weight_mismatch_refit_milp_assignment": 1.5290765521059102, "refit-weight_mismatch_refit_milp_assignment": 0.46465955549168964, "rand_score_refit_milp_assignment": 0.9471830985915493, "label_mismatch_refit_milp_assignment": 0.041666666666666664, "mse_greedy": 6.916601966662894, "r2_greedy": 0.9419967114508234, "weight_mismatch_greedy": 13.382235965729873, "refit-weight_mismatch_greedy": 12.97551313177255, "rand_score_greedy": 0.7770735524256651, "label_mismatch_greedy": 0.2791666666666667, "mse_greedy_sem": 1.8319768976187834, "r2_greedy_sem": 0.015363134255833997, "weight_mismatch_greedy_sem": 2.5815979962422198, "refit-weight_mismatch_greedy_sem": 2.727855916116804, "rand_score_greedy_sem": 0.028381570339787617, "label_mismatch_greedy_sem": 0.0422221741304743, "mse_ground_truth": 1.1823482636681615, "r2_ground_truth": 0.9895286673163225, "weight_mismatch_ground_truth": 0, "rand_score_ground_truth": 1, "label_mismatch_ground_truth": 0, "mse_baseline_sklearn": 111.31914245279756, "r2_baseline_sklearn": 0.06646697730217899, "mse_milp_val": 1.1305176230653202, "r2_milp_val": 0.9890452227735123, "label_mismatch_milp_val": 0.0, "mse_refit_milp_assignment_val": 1.1342785952196104, "r2_refit_milp_assignment_val": 0.9890087787488773, "label_mismatch_refit_milp_assignment_val": 0.0, "mse_greedy_val": 16.562915140552924, "label_mismatch_greedy_val": 0.24375, "mse_greedy_val_sem": 4.8030910374060545, "label_mismatch_greedy_val_sem": 0.04478720398005302, "r2_greedy_val": 0.8395044518686865, "r2_greedy_val_sem": 0.04654221326568663, "mse_refit_ground_truth_assignment_val": 1.253761031271065, "r2_refit_ground_truth_assignment_val": 0.9878509874480458, "label_mismatch_refit_ground_truth_assignment_val": 0.041666666666666664, "mse_ground_truth_val": 0.9609666660874341, "r2_ground_truth_val": 0.9906881807640249, "label_mismatch_ground_truth_val": 0.041666666666666664, "mse_baseline_sklearn_val": 103.5709148259953, "r2_baseline_sklearn_val": -0.0036077951495672167}
{"time_milp": 507.0591380596161, "time_greedy": 0.4730446934700012, "time_refit_milp_assignment": 509.78444743156433, "mse_refit_ground_truth_assignment": 0.804291580740748, "r2_refit_ground_truth_assignment": 0.9927460390376596, "weight_mismatch_refit_ground_truth_assignment": 0.7357436262847247, "refit-weight_mismatch_refit_ground_truth_assignment": 0.0, "rand_score_refit_ground_truth_assignment": 1.0, "label_mismatch_refit_ground_truth_assignment": 0.0, "mse_milp": 0.7593509913935546, "r2_milp": 0.993151361297093, "weight_mismatch_milp": 0.598808379653585, "refit-weight_mismatch_milp": 0.20230948300260526, "rand_score_milp": 0.9816118935837246, "label_mismatch_milp": 0.013888888888888888, "mse_refit_milp_assignment": 0.7593052347257943, "r2_refit_milp_assignment": 0.9931517739796196, "weight_mismatch_refit_milp_assignment": 0.5920039227572618, "refit-weight_mismatch_refit_milp_assignment": 0.20592040093595565, "rand_score_refit_milp_assignment": 0.9816118935837246, "label_mismatch_refit_milp_assignment": 0.013888888888888888, "mse_greedy": 6.078926733784078, "r2_greedy": 0.9451737426131144, "weight_mismatch_greedy": 11.002472471360743, "refit-weight_mismatch_greedy": 10.77455689031281, "rand_score_greedy": 0.8182902973395931, "label_mismatch_greedy": 0.22638888888888892, "mse_greedy_sem": 1.666965441401284, "r2_greedy_sem": 0.015034475713843393, "weight_mismatch_greedy_sem": 2.7902262158461353, "refit-weight_mismatch_greedy_sem": 2.821816877877115, "rand_score_greedy_sem": 0.03656434892895884, "label_mismatch_greedy_sem": 0.047229746562562375, "mse_ground_truth": 0.8002855248618094, "r2_ground_truth": 0.9928060796475773, "weight_mismatch_ground_truth": 0, "rand_score_ground_truth": 1, "label_mismatch_ground_truth": 0, "mse_baseline_sklearn": 105.65015709775506, "r2_baseline_sklearn": 0.04713398274489633, "mse_milp_val": 0.7579469450497195, "r2_milp_val": 0.993216547734431, "label_mismatch_milp_val": 0.0, "mse_refit_milp_assignment_val": 0.7581684800006118, "r2_refit_milp_assignment_val": 0.9932145650471541, "label_mismatch_refit_milp_assignment_val": 0.0, "mse_greedy_val": 14.101719887991266, "label_mismatch_greedy_val": 0.2354166666666667, "mse_greedy_val_sem": 4.541623015665052, "label_mismatch_greedy_val_sem": 0.051567622242475845, "r2_greedy_val": 0.8737928236964683, "r2_greedy_val_sem": 0.040646490016464545, "mse_refit_ground_truth_assignment_val": 1.0543862918136018, "r2_refit_ground_truth_assignment_val": 0.9905634832006366, "label_mismatch_refit_ground_truth_assignment_val": 0.020833333333333332, "mse_ground_truth_val": 0.9910157426450676, "r2_ground_truth_val": 0.9911306351604607, "label_mismatch_ground_truth_val": 0.020833333333333332, "mse_baseline_sklearn_val": 111.8431886210077, "r2_baseline_sklearn_val": -0.0009710259995352466}
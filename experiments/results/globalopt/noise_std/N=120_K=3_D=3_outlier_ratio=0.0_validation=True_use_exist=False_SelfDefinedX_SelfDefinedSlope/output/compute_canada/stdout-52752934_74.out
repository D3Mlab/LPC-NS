==================== Evaluating with noise_std = 1.1 in Dataset 1 with random state = 7 ====================
ODS is enabled
mse 1.2052021831326
Splitting data into training and validation set
Not whitening the data...
Set parameter TokenServer to value "license1.computecanada.ca"
Set parameter Threads to value 16
Read parameters from file gurobi.env
Set parameter MIPGap to value 0.05
Set parameter TimeLimit to value 36000
Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (linux64 - "AlmaLinux 9.3 (Shamrock Pampas Cat)")

CPU model: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 48 physical cores, 48 logical processors, using up to 16 threads

Optimize a model with 72 rows, 312 columns and 216 nonzeros
Model fingerprint: 0xeb098e8a
Model has 84 quadratic objective terms
Model has 216 general constraints
Variable types: 96 continuous, 216 integer (216 binary)
Coefficient statistics:
  Matrix range     [1e+00, 1e+00]
  Objective range  [0e+00, 0e+00]
  QObjective range [2e-02, 2e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 1e+00]
  GenCon rhs range [6e-01, 2e+01]
  GenCon coe range [6e-03, 4e+00]
Presolve added 216 rows and 204 columns
Presolve time: 0.03s
Presolved: 288 rows, 516 columns, 1512 nonzeros
Presolved model has 216 SOS constraint(s)
Presolved model has 84 quadratic objective terms
Variable types: 300 continuous, 216 integer (216 binary)

Root relaxation: objective 0.000000e+00, 577 iterations, 0.01 seconds (0.00 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0    0.00000    0   72          -    0.00000      -     -    0s
H    0     0                    8527.3144527    0.00000   100%     -    0s
H    0     0                    8016.7266209    0.00000   100%     -    0s
     0     0    0.17383    0   71 8016.72662    0.17383   100%     -    0s
     0     2    0.17383    0   71 8016.72662    0.17383   100%     -    0s
H   32    48                    7675.1395536    0.17383   100%   2.4    0s
H   35    48                    7542.7255324    0.17383   100%   2.5    0s
H   42    48                    7279.5576696    0.17383   100%   2.6    0s
H   79    96                    6878.4208976    0.17383   100%   2.4    0s
H  332   352                    6723.7648751    0.17383   100%   2.5    0s
H  534   640                    6492.7186387    0.17383   100%   2.5    0s
H 1343  1616                    6117.4988931    0.17383   100%   2.5    0s
H 1411  1616                    4741.4002152    0.23604   100%   2.5    0s
H 1616  1852                    2925.8403280    0.23604   100%   2.5    0s
H 1721  1852                    2913.6423542    0.23604   100%   2.5    0s
H 1742  1852                    2856.8075279    0.23604   100%   2.4    0s
H 1830  1852                    2797.0014596    0.23604   100%   2.4    0s
H 1851  1973                    1574.0831971    0.23604   100%   2.4    0s
H 1876  1973                    1472.0787885    0.23604   100%   2.4    0s
H 1901  1971                     811.4328335    0.23604   100%   2.4    0s
H 3428  3100                     793.5893229    0.23604   100%   2.3    0s
H 3431  3070                     774.5210017    0.23604   100%   2.3    0s
H 3452  3070                     764.1398606    0.23604   100%   2.3    0s
H 3464  3032                     743.4904369    0.23604   100%   2.3    0s
H 3496  3446                     742.5039080    0.23604   100%   2.3    0s
H 3499  3204                     598.1988347    0.23604   100%   2.3    0s
H 3887  3105                     544.7990726    0.23604   100%   2.2    0s
H 5938  4108                     514.2544333    0.23604   100%   2.2    0s
H 5940  3895                     512.6425292    0.23604   100%   2.2    0s
H 5942  3702                     414.1602296    0.23604   100%   2.2    1s
H 6020  3573                     390.6459220    0.23604   100%   2.3    1s
H 6028  3394                     385.8902969    0.23604   100%   2.3    1s
H 6070  3267                     374.6805880    0.23604   100%   2.3    1s
H 6748  3543                     364.2134845    0.23604   100%   2.3    1s
H 6760  3389                     363.7943450    0.23604   100%   2.3    1s
H 7833  3820                     361.1096806    0.29345   100%   2.3    1s
H22157  9980                     341.9523954    0.63279   100%   2.1    2s
H27172 12827                     337.8711824    0.70982   100%   2.1    2s
H28182 13501                     337.2393761    0.70982   100%   2.1    2s
H28278 13359                     329.9948834    0.70982   100%   2.1    2s
H29693 13924                     326.0257523    0.70982   100%   2.1    2s
 85266 45465  205.28666   76   60  326.02575    1.24735   100%   2.0    5s
H102196 54608                     325.2824567    1.41122   100%   2.0    5s
H136633 70188                     300.0591910    1.68108  99.4%   2.0    7s
H138307 53190                     153.5883521    1.68121  98.9%   2.0    7s
H140780 53166                     148.3256539    1.70502  98.9%   2.0    7s
H142925 46367                      93.8425184    1.72542  98.2%   2.0    7s
H145639 46742                      91.8872062    1.73481  98.1%   2.0    7s
H163560 54403                      90.9264471    1.87247  97.9%   2.0    8s
 244284 89655   52.85188   64   68   90.92645    2.46333  97.3%   2.0   10s
*247281 85455             133      80.6594422    2.47530  96.9%   2.0   10s
H308985 103055                      66.0277429    2.83145  95.7%   2.0   11s
H308986 95491                      56.7794614    2.83145  95.0%   2.0   11s
 432629 148070   15.21653   83   62   56.77946    3.50224  93.8%   2.0   15s
 663713 232463    8.21327   66   68   56.77946    4.65424  91.8%   1.9   20s
 862963 296655     cutoff   87        56.77946    5.49087  90.3%   1.9   25s
 1071236 361570     cutoff   88        56.77946    6.24152  89.0%   1.9   30s
 1312065 433556   55.59602   88   52   56.77946    6.99956  87.7%   1.9   35s
 1552689 496837   32.62770   90   56   56.77946    7.71930  86.4%   1.9   40s
 1800474 558519   20.40844   93   55   56.77946    8.44016  85.1%   1.9   45s
 2033448 618208     cutoff   85        56.77946    9.04977  84.1%   1.8   50s
 2261363 670378     cutoff  101        56.77946    9.64446  83.0%   1.8   55s
 2475780 718957   27.75812   82   60   56.77946   10.24141  82.0%   1.8   60s
 2684062 766430   52.01835   98   47   56.77946   10.74843  81.1%   1.8   65s
 2908491 811924     cutoff   89        56.77946   11.30983  80.1%   1.8   70s
 3134770 858320   23.51452   78   67   56.77946   11.90525  79.0%   1.8   75s
 3356327 899270     cutoff   94        56.77946   12.45731  78.1%   1.8   80s
 3567757 937347     cutoff   92        56.77946   12.98272  77.1%   1.8   85s
 3790401 975021   50.75459   85   59   56.77946   13.54018  76.2%   1.8   90s
 4014022 1010068     cutoff   84        56.77946   14.13322  75.1%   1.8   95s
 4226616 1041897     cutoff  109        56.77946   14.65454  74.2%   1.8  100s
 4454439 1074737   56.06801   98   48   56.77946   15.21175  73.2%   1.8  105s
 4668702 1105471   26.80390   85   63   56.77946   15.74441  72.3%   1.8  110s
 4877705 1133782     cutoff   79        56.77946   16.21993  71.4%   1.8  115s
 5101545 1161853     cutoff   94        56.77946   16.75436  70.5%   1.8  120s
 5318500 1189457   19.34978  102   48   56.77946   17.26495  69.6%   1.8  125s
 5527048 1213166   53.97312   91   59   56.77946   17.73494  68.8%   1.8  130s
 5739511 1234980   42.54820   96   51   56.77946   18.24244  67.9%   1.7  135s
 5952468 1256878   19.04706   79   65   56.77946   18.74519  67.0%   1.7  140s
 6155465 1274873   41.36856   88   51   56.77946   19.25305  66.1%   1.7  145s
 6369365 1293424     cutoff   79        56.77946   19.78934  65.1%   1.7  150s
 6576200 1309150     cutoff   73        56.77946   20.31536  64.2%   1.7  155s
 6792514 1328882   22.66691  100   52   56.77946   20.83321  63.3%   1.7  160s
 6997176 1347056   33.82816   95   53   56.77946   21.32115  62.4%   1.7  165s
 7200579 1361464   42.88758  100   46   56.77946   21.84148  61.5%   1.7  170s
 7408815 1375736   37.06811   82   59   56.77946   22.34680  60.6%   1.7  175s
 7617259 1390201   26.38625   86   62   56.77946   22.86274  59.7%   1.7  180s
 7815926 1402689     cutoff   81        56.77946   23.36441  58.9%   1.7  185s
 8029276 1413641   55.10174   85   64   56.77946   23.91672  57.9%   1.7  190s
 8218391 1424368   28.60298   89   59   56.77946   24.38089  57.1%   1.7  195s
 8403117 1433117     cutoff  108        56.77946   24.84699  56.2%   1.7  200s
 8597392 1441072     cutoff  100        56.77946   25.33322  55.4%   1.7  205s
 8797773 1452260   40.75539   95   47   56.77946   25.80359  54.6%   1.7  210s
 9000369 1460782   51.71629   80   57   56.77946   26.28598  53.7%   1.7  215s
 9192778 1467948   32.86107   82   58   56.77946   26.74852  52.9%   1.7  220s
 9385787 1474019     cutoff  100        56.77946   27.19955  52.1%   1.7  225s
 9583782 1481558   31.97330  101   51   56.77946   27.66043  51.3%   1.7  230s
 9772913 1488095   28.09701   92   56   56.77946   28.08571  50.5%   1.7  235s
 9992145 1493570   48.59511   97   47   56.77946   28.58275  49.7%   1.7  240s
 10213930 1500220   33.85072  101   48   56.77946   29.06816  48.8%   1.7  245s
 10426809 1503228     cutoff   96        56.77946   29.53765  48.0%   1.7  250s
 10648456 1506515     cutoff   96        56.77946   30.02072  47.1%   1.7  255s
 10868990 1508769   46.45957  104   47   56.77946   30.48055  46.3%   1.7  260s
 11075698 1510378     cutoff   89        56.77946   30.91102  45.6%   1.7  265s
 11273610 1510413   35.76536   86   64   56.77946   31.32940  44.8%   1.7  270s
 11509331 1508119   55.42112   89   55   56.77946   31.82656  43.9%   1.7  275s
 11749494 1508627   36.30773  101   48   56.77946   32.32397  43.1%   1.7  280s
 11994794 1506510   52.09476   87   56   56.77946   32.81488  42.2%   1.7  285s
 12231505 1501918     cutoff  105        56.77946   33.29537  41.4%   1.7  290s
 12480566 1496407   48.07280   99   50   56.77946   33.79755  40.5%   1.7  295s
 12717211 1487701     cutoff  100        56.77946   34.28025  39.6%   1.7  300s
 12959976 1477804   34.78047   92   51   56.77946   34.77882  38.7%   1.7  305s
 13203010 1468166     cutoff  102        56.77946   35.26001  37.9%   1.6  310s
 13444447 1457533     cutoff   83        56.77946   35.74783  37.0%   1.6  315s
 13689857 1446066   53.09014   83   59   56.77946   36.22344  36.2%   1.6  320s
 13934215 1433682     cutoff   94        56.77946   36.70117  35.4%   1.6  325s
 14170230 1420075     cutoff   90        56.77946   37.15486  34.6%   1.6  330s
 14402782 1406073     cutoff   96        56.77946   37.60554  33.8%   1.6  335s
 14617792 1391507     cutoff   94        56.77946   38.02855  33.0%   1.6  340s
 14849661 1374196   44.48646   99   52   56.77946   38.48983  32.2%   1.6  345s
 15054701 1359717   47.37936   95   56   56.77946   38.89001  31.5%   1.6  350s
 15266164 1342338   48.99839   91   51   56.77946   39.30906  30.8%   1.6  355s
 15486122 1322744   49.72811  104   44   56.77946   39.74882  30.0%   1.6  360s
 15700898 1303021   48.05047   78   62   56.77946   40.17933  29.2%   1.6  365s
 15899623 1282276   41.02990   94   52   56.77946   40.58095  28.5%   1.6  370s
 16111748 1260100   43.31304   91   53   56.77946   41.01499  27.8%   1.6  375s
 16349064 1235776   47.63374   81   61   56.77946   41.49067  26.9%   1.6  380s
 16584889 1208423     cutoff   94        56.77946   41.97623  26.1%   1.6  385s
 16822178 1179617     cutoff   85        56.77946   42.46209  25.2%   1.6  390s
 17060562 1149477     cutoff   90        56.77946   42.95626  24.3%   1.6  395s
 17297827 1116817     cutoff   97        56.77946   43.45822  23.5%   1.6  400s
 17531936 1084267   55.17181   85   57   56.77946   43.94872  22.6%   1.6  405s
 17734616 1054415     cutoff   95        56.77946   44.38240  21.8%   1.6  410s
 17952005 1020322     cutoff  107        56.77946   44.84846  21.0%   1.6  415s
 18166983 985409   56.53536   88   57   56.77946   45.32012  20.2%   1.6  420s
 18367491 948840     cutoff  104        56.77946   45.78182  19.4%   1.6  425s
 18576315 909748     cutoff   96        56.77946   46.26386  18.5%   1.6  430s
 18775394 871536   49.46420   91   52   56.77946   46.71737  17.7%   1.6  435s
 18983708 830569   52.18362   97   50   56.77946   47.20011  16.9%   1.6  440s
 19176316 791258   52.07845   73   60   56.77946   47.65681  16.1%   1.6  445s
 19371428 749297     cutoff   96        56.77946   48.13130  15.2%   1.6  450s
 19566937 705186     cutoff   73        56.77946   48.61317  14.4%   1.6  455s
 19761377 659318   56.63013   94   50   56.77946   49.10182  13.5%   1.6  460s
 19947260 614429   52.02752   99   48   56.77946   49.56947  12.7%   1.6  465s
 20161037 561901   54.41877  100   48   56.77946   50.11306  11.7%   1.6  470s
 20390008 501380   55.87655   76   63   56.77946   50.71927  10.7%   1.6  475s
 20616771 437366     cutoff   99        56.77946   51.35898  9.55%   1.6  480s
 20835771 371277   54.24725   83   61   56.77946   52.02341  8.38%   1.6  485s
 21044745 301834   52.85831   76   64   56.77946   52.72644  7.14%   1.6  490s
 21270386 220146   53.62209   81   59   56.77946   53.61723  5.57%   1.6  495s

Explored 21343712 nodes (33712571 simplex iterations) in 496.62 seconds (374.27 work units)
Thread count was 16 (of 48 available processors)

Solution count 10: 56.7795 66.0277 80.6594 ... 325.282

Optimal solution found (tolerance 5.00e-02)
Best objective 5.677946140884e+01, best bound 5.394685461651e+01, gap 4.9888%
Optimal solution found.
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 1 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1]
Cluster Assignments from Gurobi MIQP using MSE

Cluster 0 weights w_0^* (including bias) in original space:
[-9.67744241 -1.72083342 -1.90210549 -0.95273021]

Cluster 1 weights w_1^* (including bias) in original space:
[10.08995144 -0.1239494   0.07369482  0.25392117]

Cluster 2 weights w_2^* (including bias) in original space:
[1.28724841 1.44402066 1.29998704 1.13376533]
############################################################################

Ridge Regression using scikit-learn:
Optimal weights w* in X Space (including bias):
[ 3.95863167e-01  6.92719759e-05 -6.64080414e-04  4.00576316e-04]

Mean Squared Error on training data (scikit-learn) with lambda = 0.1:
118.43466883368855
Cluster assignments:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 1 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1]
-----------------------------------
Regression weights for cluster 0: y = -1.7208x_0 + -1.9021x_1 + -0.9527x_2 + -9.6774
Regression weights for cluster 0 after refit: y = -1.7162x_1 + -1.8984x_2 + -0.9483x_3 + -9.6947
-----------------------------------
Regression weights for cluster 1: y = -0.1239x_0 + 0.0737x_1 + 0.2539x_2 + 10.09
Regression weights for cluster 1 after refit: y = -0.1286x_1 + 0.0685x_2 + 0.251x_3 + 10.1064
-----------------------------------
Regression weights for cluster 2: y = 1.444x_0 + 1.3x_1 + 1.1338x_2 + 1.2872
Regression weights for cluster 2 after refit: y = 1.4442x_1 + 1.3002x_2 + 1.1334x_3 + 1.2879
{'time_milp': 497.70627641677856, 'time_greedy': np.float64(0.5038382530212402), 'time_refit_milp_assignment': 500.6622166633606, 'mse_refit_ground_truth_assignment': np.float64(0.8378699519751347), 'r2_refit_ground_truth_assignment': 0.9929931378984134, 'weight_mismatch_refit_ground_truth_assignment': np.float64(1.2145567497184728), 'refit-weight_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'rand_score_refit_ground_truth_assignment': 1.0, 'label_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'mse_milp': np.float64(0.7594717078880736), 'r2_milp': 0.9936487595542915, 'weight_mismatch_milp': np.float64(1.2984505842931116), 'refit-weight_mismatch_milp': np.float64(0.32520664143952915), 'rand_score_milp': np.float64(0.9640062597809077), 'label_mismatch_milp': np.float64(0.027777777777777776), 'mse_refit_milp_assignment': np.float64(0.7594280419783676), 'r2_refit_milp_assignment': 0.9936491247195623, 'weight_mismatch_refit_milp_assignment': np.float64(1.2931356142245143), 'refit-weight_mismatch_refit_milp_assignment': np.float64(0.2911031492740338), 'rand_score_refit_milp_assignment': np.float64(0.9640062597809077), 'label_mismatch_refit_milp_assignment': np.float64(0.027777777777777776), 'mse_greedy': np.float64(6.328041224573753), 'r2_greedy': np.float64(0.9470804363741407), 'weight_mismatch_greedy': np.float64(14.662304996661613), 'refit-weight_mismatch_greedy': np.float64(14.294568180259057), 'rand_score_greedy': np.float64(0.8016236306729265), 'label_mismatch_greedy': np.float64(0.22777777777777777), 'mse_greedy_sem': np.float64(1.8036294957430032), 'r2_greedy_sem': np.float64(0.015083227569187915), 'weight_mismatch_greedy_sem': np.float64(3.1428633767268015), 'refit-weight_mismatch_greedy_sem': np.float64(3.1919546357092474), 'rand_score_greedy_sem': np.float64(0.028877506323069707), 'label_mismatch_greedy_sem': np.float64(0.03887322157251185), 'mse_ground_truth': np.float64(1.2052021831326), 'r2_ground_truth': np.float64(0.9895252573764585), 'weight_mismatch_ground_truth': 0, 'rand_score_ground_truth': 1, 'label_mismatch_ground_truth': 0, 'mse_baseline_sklearn': np.float64(118.43466883368855), 'r2_baseline_sklearn': np.float64(0.009565397818006027), 'mse_milp_val': np.float64(1.6335872493527184), 'r2_milp_val': 0.984861410857832, 'label_mismatch_milp_val': np.float64(0.020833333333333332), 'mse_refit_milp_assignment_val': np.float64(1.6328191121442568), 'r2_refit_milp_assignment_val': 0.9848685292493402, 'label_mismatch_refit_milp_assignment_val': np.float64(0.020833333333333332), 'mse_greedy_val': np.float64(17.67041269211248), 'label_mismatch_greedy_val': np.float64(0.215625), 'mse_greedy_val_sem': np.float64(6.0783729024495115), 'label_mismatch_greedy_val_sem': np.float64(0.033226081567723706), 'r2_greedy_val': np.float64(0.8362468133707359), 'r2_greedy_val_sem': np.float64(0.05632878810699029), 'mse_refit_ground_truth_assignment_val': np.float64(1.7747801465212802), 'r2_refit_ground_truth_assignment_val': 0.9835529645162765, 'label_mismatch_refit_ground_truth_assignment_val': np.float64(0.041666666666666664), 'mse_ground_truth_val': np.float64(1.6973311228685357), 'r2_ground_truth_val': 0.9842706910711364, 'label_mismatch_ground_truth_val': np.float64(0.041666666666666664), 'mse_baseline_sklearn_val': np.float64(108.52664119301117), 'r2_baseline_sklearn_val': -0.0057254258390355695}

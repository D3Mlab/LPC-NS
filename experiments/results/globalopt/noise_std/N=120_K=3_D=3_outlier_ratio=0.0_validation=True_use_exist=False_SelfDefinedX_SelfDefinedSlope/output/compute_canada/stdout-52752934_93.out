==================== Evaluating with noise_std = 0.9 in Dataset 1 with random state = 9 ====================
ODS is enabled
mse 0.8556818790212939
Splitting data into training and validation set
Not whitening the data...
Set parameter TokenServer to value "license1.computecanada.ca"
Set parameter Threads to value 16
Read parameters from file gurobi.env
Set parameter MIPGap to value 0.05
Set parameter TimeLimit to value 36000
Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (linux64 - "AlmaLinux 9.3 (Shamrock Pampas Cat)")

CPU model: Intel(R) Xeon(R) Platinum 8160 CPU @ 2.10GHz, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 48 physical cores, 48 logical processors, using up to 16 threads

Optimize a model with 72 rows, 312 columns and 216 nonzeros
Model fingerprint: 0xa161413a
Model has 84 quadratic objective terms
Model has 216 general constraints
Variable types: 96 continuous, 216 integer (216 binary)
Coefficient statistics:
  Matrix range     [1e+00, 1e+00]
  Objective range  [0e+00, 0e+00]
  QObjective range [2e-02, 2e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 1e+00]
  GenCon rhs range [1e-01, 2e+01]
  GenCon coe range [6e-03, 5e+00]
Presolve added 216 rows and 204 columns
Presolve time: 0.00s
Presolved: 288 rows, 516 columns, 1512 nonzeros
Presolved model has 216 SOS constraint(s)
Presolved model has 84 quadratic objective terms
Variable types: 300 continuous, 216 integer (216 binary)

Root relaxation: objective 0.000000e+00, 577 iterations, 0.00 seconds (0.00 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0    0.00000    0   72          -    0.00000      -     -    0s
H    0     0                    7982.9542310    0.00000   100%     -    0s
H    0     0                    7315.1734177    0.00000   100%     -    0s
     0     0    0.18413    0   71 7315.17342    0.18413   100%     -    0s
     0     2    0.18413    0   71 7315.17342    0.18413   100%     -    0s
H   32    48                    7272.8887168    0.18413   100%   2.4    0s
H   35    48                    6867.3899604    0.18413   100%   2.5    0s
H   42    48                    6646.3285571    0.18413   100%   2.6    0s
H   46    48                    6480.4736953    0.18413   100%   2.6    0s
H   79    96                    6225.7381507    0.18413   100%   2.4    0s
H  443   523                    5268.4040772    0.18413   100%   2.5    0s
H 1610  1638                    5082.1594288    0.23277   100%   2.5    0s
H 1611  1638                    4463.3626078    0.23277   100%   2.5    0s
H 1614  1638                    3516.1660328    0.23277   100%   2.5    0s
H 1618  1638                    3004.3821936    0.23277   100%   2.5    0s
H 1761  1917                    2808.9745617    0.23277   100%   2.4    0s
H 1763  1917                    2090.7581825    0.23277   100%   2.4    0s
H 3324  3424                    2010.7921118    0.23277   100%   2.3    0s
H 3325  3198                    1048.1366558    0.23277   100%   2.3    0s
H 3370  3162                     962.2984394    0.23277   100%   2.3    0s
H 3422  3154                     944.1244655    0.23277   100%   2.3    0s
H 3540  3480                     783.1746492    0.23277   100%   2.3    0s
H 3707  3443                     743.4651320    0.23277   100%   2.3    0s
H 3826  3156                     499.1630003    0.23277   100%   2.3    0s
H 5957  3748                     478.0794300    0.27197   100%   2.2    0s
H 5997  3589                     475.3846700    0.27197   100%   2.3    0s
H 5999  3411                     448.1209606    0.27197   100%   2.3    0s
H 6041  3268                     443.3985855    0.27197   100%   2.3    0s
H 6174  3192                     441.9610037    0.27197   100%   2.3    0s
H 6812  3400                     439.6948922    0.27197   100%   2.3    1s
H 7260  3644                     419.3872724    0.27197   100%   2.3    1s
H 8523  4143                     410.6271458    0.27197   100%   2.3    1s
H 8524  4012                     410.2596850    0.27197   100%   2.3    1s
H10818  4348                     380.5278673    0.27197   100%   2.2    1s
H11863  4841                     361.7475752    0.28193   100%   2.2    1s
H11930  4594                     331.1918959    0.28220   100%   2.2    1s
H21934 10100                     328.1144303    0.52354   100%   2.1    1s
H25503 11614                     320.5473511    0.59541   100%   2.1    1s
H25521 11610                     319.2767196    0.59541   100%   2.1    1s
H34440 16536                     319.2760740    0.81837   100%   2.1    2s
H34548 16532                     318.8508292    0.81837   100%   2.1    2s
H34656 16503                     316.5937310    0.81837   100%   2.1    2s
H34759 16480                     315.6886699    0.81837   100%   2.1    2s
H40912 20011                     312.8685746    0.87402   100%   2.0    2s
H41446 20028                     312.7340958    0.87402   100%   2.0    2s
H41465 20027                     312.6760633    0.87402   100%   2.0    2s
 96886 51848   21.38146   89   64  312.67606    1.35047   100%   2.0    5s
H204478 104054                     283.1608466    1.86836  99.3%   2.0    9s
H205574 96739                     240.0929943    1.86836  99.2%   2.0    9s
H207353 94622                     225.3600284    1.86836  99.2%   2.0    9s
H207354 91898                     210.9623947    1.86836  99.1%   2.0    9s
H207356 86533                     184.7794516    1.86836  99.0%   2.0    9s
H207370 84708                     175.2712048    1.86836  98.9%   2.0    9s
H208660 77452                     141.0287666    1.88687  98.7%   2.0    9s
H210781 76726                     134.4619260    1.88849  98.6%   2.0    9s
H210794 76524                     133.7924992    1.88849  98.6%   2.0    9s
H213134 77019                     131.8280592    1.89273  98.6%   2.0    9s
H213209 74069                     118.8055955    1.89273  98.4%   2.0    9s
 215356 75944     cutoff   93       118.80560    1.90961  98.4%   2.0   10s
 442986 182508     cutoff   76       118.80560    2.74879  97.7%   2.0   15s
H474061 195641                     118.8014015    2.85901  97.6%   2.0   15s
H474176 195512                     118.5623958    2.85901  97.6%   2.0   15s
H476218 186366                     106.0916466    2.88009  97.3%   2.0   16s
H478011 177428                      94.9331068    2.88364  97.0%   2.0   16s
H480092 174026                      90.3003087    2.89782  96.8%   2.0   16s
H481814 172204                      87.8571497    2.90297  96.7%   2.0   16s
H481837 169195                      84.6457755    2.90297  96.6%   2.0   16s
H483594 169731                      84.3043675    2.91016  96.5%   2.0   16s
H483601 165500                      80.1281669    2.91016  96.4%   2.0   16s
H485883 157565                      71.6355196    2.91670  95.9%   2.0   16s
H485898 155333                      69.2797811    2.91670  95.8%   2.0   16s
H487843 146643                      60.2328686    2.92123  95.2%   2.0   16s
H489938 144719                      57.9764374    2.92673  95.0%   2.0   16s
H489942 143248                      56.6574504    2.92673  94.8%   2.0   16s
H489943 142670                      56.1512968    2.92673  94.8%   2.0   16s
H491842 137993                      51.5917799    2.93235  94.3%   2.0   17s
 612309 182931     cutoff   75        51.59178    3.39606  93.4%   2.0   20s
 843555 268353     cutoff   98        51.59178    4.08462  92.1%   1.9   25s
 1063677 342154   50.66175   84   58   51.59178    4.69047  90.9%   1.9   30s
 1249147 395656   31.89987   84   60   51.59178    5.18370  90.0%   1.9   35s
 1461907 456216   29.55057   66   68   51.59178    5.68604  89.0%   1.9   40s
 1692229 514272   49.13987  100   47   51.59178    6.26117  87.9%   1.9   45s
 1920126 572052     cutoff   79        51.59178    6.75633  86.9%   1.9   50s
 2148751 627636   51.41570   77   63   51.59178    7.26615  85.9%   1.8   55s
 2391280 684341     cutoff  102        51.59178    7.77966  84.9%   1.8   60s
 2637260 736515   45.29277   80   58   51.59178    8.33796  83.8%   1.8   65s
 2859711 780345   17.14288   94   51   51.59178    8.85183  82.8%   1.8   70s
 3077395 824400     cutoff   91        51.59178    9.38362  81.8%   1.8   75s
 3298297 867837   23.36061   85   59   51.59178    9.92729  80.8%   1.8   80s
 3517007 910761   41.40881   79   61   51.59178   10.47043  79.7%   1.8   85s
 3729655 946328   20.92781   83   62   51.59178   11.01542  78.6%   1.8   90s
 3927046 975260   29.89217  102   50   51.59178   11.55812  77.6%   1.8   95s
 4144956 1001920   16.09935   89   57   51.59178   12.17779  76.4%   1.8  100s
 4353533 1029845     cutoff   82        51.59178   12.73594  75.3%   1.8  105s
 4557059 1057250     cutoff   98        51.59178   13.26418  74.3%   1.8  110s
 4762118 1081772   21.34199   53   68   51.59178   13.83236  73.2%   1.8  115s
 4961519 1099039   14.39382   75   64   51.59178   14.39382  72.1%   1.7  120s
 5155039 1119000     cutoff   73        51.59178   14.91083  71.1%   1.7  125s
 5348710 1135164   50.46064   77   58   51.59178   15.42726  70.1%   1.7  130s
 5549586 1153344   21.71025  100   48   51.59178   15.94050  69.1%   1.7  135s
 5742445 1168734     cutoff  108        51.59178   16.42770  68.2%   1.7  140s
 5950787 1181315     cutoff   76        51.59178   16.96250  67.1%   1.7  145s
 6160124 1195548     cutoff   71        51.59178   17.48336  66.1%   1.7  150s
 6371700 1209427   47.25148   83   63   51.59178   18.00946  65.1%   1.7  155s
 6577479 1220411     cutoff   90        51.59178   18.50833  64.1%   1.7  160s
 6790861 1228935     cutoff   91        51.59178   19.03771  63.1%   1.7  165s
 6999841 1237474   46.62150   76   60   51.59178   19.56661  62.1%   1.7  170s
 7215860 1245223   47.28229   87   58   51.59178   20.11433  61.0%   1.7  175s
 7405522 1253192   46.32775   88   58   51.59178   20.56729  60.1%   1.7  180s
 7617589 1260327   21.09711   89   54   51.59178   21.09025  59.1%   1.7  185s
 7816157 1265924     cutoff   87        51.59178   21.57876  58.2%   1.7  190s
 8019951 1268865   41.74543   92   55   51.59178   22.09823  57.2%   1.7  195s
 8211540 1268171   28.87991   93   56   51.59178   22.60342  56.2%   1.7  200s
 8401815 1268775     cutoff  105        51.59178   23.10447  55.2%   1.7  205s
 8592116 1268339   35.39621   80   61   51.59178   23.60876  54.2%   1.7  210s
 8798144 1269087   50.39470   96   48   51.59178   24.14643  53.2%   1.7  215s
 8998737 1269305     cutoff   89        51.59178   24.67247  52.2%   1.7  220s
 9192366 1267072   25.60553   80   66   51.59178   25.19456  51.2%   1.7  225s
 9398030 1263228   32.24767   79   64   51.59178   25.74686  50.1%   1.7  230s
 9617791 1260112   47.31055   83   60   51.59178   26.31203  49.0%   1.7  235s
 9832568 1255143     cutoff   91        51.59178   26.86724  47.9%   1.7  240s
 10052185 1250036   31.14362   73   59   51.59178   27.42203  46.8%   1.7  245s
 10264274 1241230   41.56817   84   55   51.59178   27.98616  45.8%   1.7  250s
 10473683 1234527     cutoff  105        51.59178   28.53131  44.7%   1.7  255s
 10681167 1225475     cutoff   84        51.59178   29.06909  43.7%   1.7  260s
 10886440 1217349     cutoff   85        51.59178   29.60369  42.6%   1.7  265s
 11076817 1209489     cutoff   67        51.59178   30.08714  41.7%   1.7  270s
 11272406 1200140     cutoff   94        51.59178   30.58581  40.7%   1.6  275s
 11464149 1189042   46.03885   97   47   51.59178   31.07722  39.8%   1.6  280s
 11649262 1178997   38.09001   68   55   51.59178   31.53125  38.9%   1.6  285s
 11837003 1164934     cutoff  100        51.59178   32.01143  38.0%   1.6  290s
 12034211 1149379   38.50777   74   64   51.59178   32.51330  37.0%   1.6  295s
 12237470 1134666     cutoff   86        51.59178   33.02761  36.0%   1.6  300s
 12424552 1116900     cutoff  101        51.59178   33.50938  35.0%   1.6  305s
 12633204 1097828     cutoff  103        51.59178   34.04189  34.0%   1.6  310s
 12846199 1077026   49.62391  100   41   51.59178   34.58618  33.0%   1.6  315s
 13057331 1053515   36.40041   85   61   51.59178   35.12583  31.9%   1.6  320s
 13276017 1026567     cutoff  102        51.59178   35.70602  30.8%   1.6  325s
 13486660 999234     cutoff   90        51.59178   36.26920  29.7%   1.6  330s
 13697900 972016     cutoff  104        51.59178   36.83508  28.6%   1.6  335s
 13897526 944864     cutoff   94        51.59178   37.36294  27.6%   1.6  340s
 14100901 916740     cutoff  107        51.59178   37.90691  26.5%   1.6  345s
 14309313 883771     cutoff   80        51.59178   38.48525  25.4%   1.6  350s
 14512972 850557     cutoff  102        51.59178   39.04550  24.3%   1.6  355s
 14721731 816244     cutoff   92        51.59178   39.62279  23.2%   1.6  360s
 14913536 781110   40.60455   92   53   51.59178   40.17845  22.1%   1.6  365s
 15120256 742263   43.06674   89   56   51.59178   40.79517  20.9%   1.6  370s
 15323064 700055   45.30671   83   59   51.59178   41.39843  19.8%   1.6  375s
 15505627 660405     cutoff   99        51.59178   41.96099  18.7%   1.6  380s
 15704665 613613   43.80589   88   54   51.59178   42.61092  17.4%   1.6  385s
 15902888 566577     cutoff   90        51.59178   43.26466  16.1%   1.6  390s
 16119279 511329   50.78281   95   50   51.59178   44.00664  14.7%   1.6  395s
 16329573 456066     cutoff   77        51.59178   44.74858  13.3%   1.6  400s
 16535200 396612     cutoff   88        51.59178   45.53008  11.7%   1.6  405s
 16753215 328497     cutoff   79        51.59178   46.43072  10.0%   1.6  410s
 16960597 258402     cutoff   57        51.59178   47.37089  8.18%   1.6  415s
 17176482 178846   50.26295   85   52   51.59178   48.49354  6.01%   1.6  420s

Explored 17264939 nodes (27329918 simplex iterations) in 422.14 seconds (298.57 work units)
Thread count was 16 (of 48 available processors)

Solution count 10: 51.5918 56.1513 56.6575 ... 84.6458

Optimal solution found (tolerance 5.00e-02)
Best objective 5.159177990967e+01, best bound 4.901332667389e+01, gap 4.9978%
Optimal solution found.
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
Cluster Assignments from Gurobi MIQP using MSE

Cluster 0 weights w_0^* (including bias) in original space:
[-10.16633745  -1.30687425  -1.58712658  -0.88004388]

Cluster 1 weights w_1^* (including bias) in original space:
[ 9.94306943 -0.08240954  0.15773447  0.43216536]

Cluster 2 weights w_2^* (including bias) in original space:
[0.99360143 1.77688924 1.65177889 0.55930803]
############################################################################

Ridge Regression using scikit-learn:
Optimal weights w* in X Space (including bias):
[ 0.35564464  0.00107695 -0.00048884 -0.00053113]

Mean Squared Error on training data (scikit-learn) with lambda = 0.1:
110.8737888343771
Cluster assignments:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
-----------------------------------
Regression weights for cluster 0: y = -1.3069x_0 + -1.5871x_1 + -0.88x_2 + -10.1663
Regression weights for cluster 0 after refit: y = -1.3035x_1 + -1.5831x_2 + -0.873x_3 + -10.1865
-----------------------------------
Regression weights for cluster 1: y = -0.0824x_0 + 0.1577x_1 + 0.4322x_2 + 9.9431
Regression weights for cluster 1 after refit: y = -0.0864x_1 + 0.1548x_2 + 0.4296x_3 + 9.9563
-----------------------------------
Regression weights for cluster 2: y = 1.7769x_0 + 1.6518x_1 + 0.5593x_2 + 0.9936
Regression weights for cluster 2 after refit: y = 1.7772x_1 + 1.6524x_2 + 0.5588x_3 + 0.9938
{'time_milp': 423.0632827281952, 'time_greedy': np.float64(0.5616300463676452), 'time_refit_milp_assignment': 426.20631742477417, 'mse_refit_ground_truth_assignment': np.float64(0.6866990704946357), 'r2_refit_ground_truth_assignment': 0.9939846209144597, 'weight_mismatch_refit_ground_truth_assignment': np.float64(1.1622350951670897), 'refit-weight_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'rand_score_refit_ground_truth_assignment': 1.0, 'label_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'mse_milp': np.float64(0.6867435221085995), 'r2_milp': 0.9939842315251035, 'weight_mismatch_milp': np.float64(1.1443473822829497), 'refit-weight_mismatch_milp': np.float64(0.03730793083784246), 'rand_score_milp': 1.0, 'label_mismatch_milp': np.float64(0.0), 'mse_refit_milp_assignment': np.float64(0.6866990704946357), 'r2_refit_milp_assignment': 0.9939846209144597, 'weight_mismatch_refit_milp_assignment': np.float64(1.1622350951670897), 'refit-weight_mismatch_refit_milp_assignment': np.float64(0.0), 'rand_score_refit_milp_assignment': 1.0, 'label_mismatch_refit_milp_assignment': np.float64(0.0), 'mse_greedy': np.float64(4.869987632559148), 'r2_greedy': np.float64(0.9573396513692165), 'weight_mismatch_greedy': np.float64(12.314254160038017), 'refit-weight_mismatch_greedy': np.float64(11.725993618479892), 'rand_score_greedy': np.float64(0.8421165884194053), 'label_mismatch_greedy': np.float64(0.20763888888888887), 'mse_greedy_sem': np.float64(1.4177062401769798), 'r2_greedy_sem': np.float64(0.012418890359728797), 'weight_mismatch_greedy_sem': np.float64(3.608573168464832), 'refit-weight_mismatch_greedy_sem': np.float64(3.7066122182387553), 'rand_score_greedy_sem': np.float64(0.03254125504274634), 'label_mismatch_greedy_sem': np.float64(0.0474803210663534), 'mse_ground_truth': np.float64(0.8556818790212939), 'r2_ground_truth': np.float64(0.9923040528609511), 'weight_mismatch_ground_truth': 0, 'rand_score_ground_truth': 1, 'label_mismatch_ground_truth': 0, 'mse_baseline_sklearn': np.float64(110.8737888343771), 'r2_baseline_sklearn': np.float64(0.02876252619868347), 'mse_milp_val': np.float64(0.8501212277913375), 'r2_milp_val': 0.992002398964304, 'label_mismatch_milp_val': np.float64(0.020833333333333332), 'mse_refit_milp_assignment_val': np.float64(0.8532561273894804), 'r2_refit_milp_assignment_val': 0.991972907080731, 'label_mismatch_refit_milp_assignment_val': np.float64(0.020833333333333332), 'mse_greedy_val': np.float64(9.344191859477798), 'label_mismatch_greedy_val': np.float64(0.2010416666666667), 'mse_greedy_val_sem': np.float64(3.6398124075296825), 'label_mismatch_greedy_val_sem': np.float64(0.043049291541127396), 'r2_greedy_val': np.float64(0.9120935743632028), 'r2_greedy_val_sem': np.float64(0.03424190165892861), 'mse_refit_ground_truth_assignment_val': np.float64(0.8532561273894804), 'r2_refit_ground_truth_assignment_val': 0.991972907080731, 'label_mismatch_refit_ground_truth_assignment_val': np.float64(0.020833333333333332), 'mse_ground_truth_val': np.float64(0.8204213039419278), 'r2_ground_truth_val': 0.9922818039879325, 'label_mismatch_ground_truth_val': np.float64(0.020833333333333332), 'mse_baseline_sklearn_val': np.float64(107.02164561949185), 'r2_baseline_sklearn_val': -0.006816905480715452}

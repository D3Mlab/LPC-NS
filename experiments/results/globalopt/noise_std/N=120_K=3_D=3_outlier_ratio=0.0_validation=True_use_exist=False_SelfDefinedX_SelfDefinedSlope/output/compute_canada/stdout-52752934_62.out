==================== Evaluating with noise_std = 0.6 in Dataset 1 with random state = 6 ====================
ODS is enabled
mse 0.3556824554941374
Splitting data into training and validation set
Not whitening the data...
Set parameter TokenServer to value "license1.computecanada.ca"
Set parameter Threads to value 16
Read parameters from file gurobi.env
Set parameter MIPGap to value 0.05
Set parameter TimeLimit to value 36000
Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (linux64 - "AlmaLinux 9.3 (Shamrock Pampas Cat)")

CPU model: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 48 physical cores, 48 logical processors, using up to 16 threads

Optimize a model with 72 rows, 312 columns and 216 nonzeros
Model fingerprint: 0x50a624fe
Model has 84 quadratic objective terms
Model has 216 general constraints
Variable types: 96 continuous, 216 integer (216 binary)
Coefficient statistics:
  Matrix range     [1e+00, 1e+00]
  Objective range  [0e+00, 0e+00]
  QObjective range [2e-02, 2e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 1e+00]
  GenCon rhs range [3e-01, 2e+01]
  GenCon coe range [6e-03, 4e+00]
Presolve added 216 rows and 204 columns
Presolve time: 0.02s
Presolved: 288 rows, 516 columns, 1512 nonzeros
Presolved model has 216 SOS constraint(s)
Presolved model has 84 quadratic objective terms
Variable types: 300 continuous, 216 integer (216 binary)

Root relaxation: objective 0.000000e+00, 577 iterations, 0.01 seconds (0.00 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0    0.00000    0   72          -    0.00000      -     -    0s
H    0     0                    7556.7311843    0.00000   100%     -    0s
H    0     0                    6647.4086130    0.00000   100%     -    0s
     0     0    0.36352    0   71 6647.40861    0.36352   100%     -    0s
     0     2    0.36352    0   71 6647.40861    0.36352   100%     -    0s
H   32    48                    6625.7009175    0.36352   100%   2.4    0s
H   35    48                    6285.0902209    0.36352   100%   2.5    0s
H   42    48                    6094.7326564    0.36352   100%   2.6    0s
H   79    96                    5909.3486634    0.36352   100%   2.4    0s
H  431   528                    5852.3124563    0.36352   100%   2.5    0s
H  491   528                    5783.7644360    0.36352   100%   2.5    0s
H  521   528                    5042.2103127    0.36352   100%   2.5    0s
H 1615  1632                    4569.5782962    0.36941   100%   2.5    0s
H 1618  1632                    4489.7604112    0.36941   100%   2.5    0s
H 1619  1632                    3811.0852352    0.36941   100%   2.5    0s
H 1631  1808                    3588.1902599    0.36941   100%   2.5    0s
H 1632  1808                    3549.2861478    0.36941   100%   2.5    0s
H 1633  1808                    3251.3236663    0.36941   100%   2.5    0s
H 1634  1808                    2951.9763280    0.36941   100%   2.5    0s
H 1655  1808                    2296.5371800    0.36941   100%   2.5    0s
H 1677  1808                    2223.3887083    0.36941   100%   2.5    0s
H 2175  2304                    1125.8313298    0.36941   100%   2.4    0s
H 2242  2300                     626.9086814    0.36941   100%   2.4    0s
* 4459  3424             138     379.9774472    0.36941   100%   2.3    0s
* 5357  3871             137     363.3809969    0.36941   100%   2.2    0s
H 5703  3760                     328.2612077    0.36941   100%   2.2    0s
H15206  6869                     322.1978789    0.48270   100%   2.2    1s
H19880  9587                     312.6437477    0.51210   100%   2.2    1s
H19883  9503                     300.8846263    0.51210   100%   2.2    1s
H19889  9122                     273.3056280    0.51210   100%   2.2    1s
H27043 12542                     257.8709354    0.57908   100%   2.1    1s
H27044 12101                     234.2415818    0.57908   100%   2.1    1s
H27046 11469                     189.2385958    0.57908   100%   2.1    1s
H27052 11245                     172.5093171    0.57908   100%   2.1    1s
H27064 10661                     140.9803349    0.57908   100%   2.1    1s
H27067 10661                     140.9254002    0.57908   100%   2.1    1s
H27078 10354                     120.7998026    0.57908   100%   2.1    1s
H41845 18159                     116.5548326    0.70205  99.4%   2.1    2s
H43176 16916                      71.5437791    0.70512  99.0%   2.1    2s
H43187 13007                      29.8959289    0.70512  97.6%   2.1    2s
H111343 38865                      29.6809792    1.36357  95.4%   2.0    3s
H113471 38634                      27.8641229    1.37125  95.1%   2.0    3s
 163063 57590     cutoff   86        27.86412    1.61344  94.2%   2.0    5s
 403065 141281   27.55750   86   66   27.86412    2.66428  90.4%   2.0   10s
 640308 205564   18.65921   85   58   27.86412    3.44342  87.6%   1.9   15s
 896186 262702    5.83540   74   65   27.86412    4.20549  84.9%   1.9   20s
 1160658 317856   11.40244   85   64   27.86412    4.90739  82.4%   1.9   25s
 1390948 357674    6.16704   72   63   27.86412    5.51386  80.2%   1.8   30s
 1635665 397030   12.44515   83   63   27.86412    6.12795  78.0%   1.8   35s
 1873419 434002     cutoff   83        27.86412    6.64261  76.2%   1.8   40s
 2087786 463799     cutoff   96        27.86412    7.07979  74.6%   1.8   45s
 2311200 491460   13.20800   91   54   27.86412    7.49147  73.1%   1.8   50s
 2539665 515017   13.78477   96   53   27.86412    7.95286  71.5%   1.8   55s
 2769335 536490   22.20263   84   63   27.86412    8.41664  69.8%   1.8   60s
 3011712 560304     cutoff   85        27.86412    8.90025  68.1%   1.8   65s
 3229517 576385     cutoff   98        27.86412    9.33615  66.5%   1.8   70s
 3456912 589338     cutoff  104        27.86412    9.79831  64.8%   1.7   75s
 3684745 603407     cutoff   82        27.86412   10.26846  63.1%   1.7   80s
 3895804 614025   10.97765   81   66   27.86412   10.70514  61.6%   1.7   85s
 4111915 616960   21.90651  103   45   27.86412   11.20011  59.8%   1.7   90s
 4343120 616461     cutoff   91        27.86412   11.75437  57.8%   1.7   95s
 4568696 616473     cutoff  104        27.86412   12.30250  55.8%   1.7  100s
 4792026 611933   22.33134  102   50   27.86412   12.87347  53.8%   1.7  105s
 5024753 602891     cutoff   87        27.86412   13.51920  51.5%   1.7  110s
 5254591 593785   19.73355   87   58   27.86412   14.19514  49.1%   1.7  115s
 5473401 583875     cutoff  107        27.86412   14.85817  46.7%   1.7  120s
 5706009 567960     cutoff  107        27.86412   15.59862  44.0%   1.7  125s
 5930197 552588     cutoff   99        27.86412   16.30234  41.5%   1.7  130s
 6162324 534309     cutoff  102        27.86412   17.03446  38.9%   1.7  135s
 6385060 513715   19.70877  103   47   27.86412   17.73709  36.3%   1.7  140s
 6618063 487478   25.58365   99   51   27.86412   18.48593  33.7%   1.7  145s
 6844979 461383   24.61020   95   47   27.86412   19.19866  31.1%   1.6  150s
 7072335 429328     cutoff   87        27.86412   19.95516  28.4%   1.6  155s
 7305787 390543   21.58441   88   60   27.86412   20.75325  25.5%   1.6  160s
 7536140 350490   22.61921  104   47   27.86412   21.55942  22.6%   1.6  165s
 7763149 305834     cutoff  103        27.86412   22.39220  19.6%   1.6  170s
 7992822 255487   23.28077   79   66   27.86412   23.27261  16.5%   1.6  175s
 8220290 197811     cutoff   49        27.86412   24.20449  13.1%   1.6  180s
 8445415 130201     cutoff   94        27.86412   25.26684  9.32%   1.6  185s

Explored 8640663 nodes (13842925 simplex iterations) in 189.49 seconds (151.71 work units)
Thread count was 16 (of 48 available processors)

Solution count 10: 27.8641 29.681 29.8959 ... 189.239

Optimal solution found (tolerance 5.00e-02)
Best objective 2.786412293035e+01, best bound 2.648248843643e+01, gap 4.9585%
Optimal solution found.
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
Cluster Assignments from Gurobi MIQP using MSE

Cluster 0 weights w_0^* (including bias) in original space:
[-10.09069201  -1.28543318  -1.61461885  -1.08634229]

Cluster 1 weights w_1^* (including bias) in original space:
[9.87927868 0.21939854 0.17592967 0.09631285]

Cluster 2 weights w_2^* (including bias) in original space:
[1.00036642 1.55039084 1.73992393 0.98358682]
############################################################################

Ridge Regression using scikit-learn:
Optimal weights w* in X Space (including bias):
[ 5.92575864e-01  2.93526683e-04  1.01514154e-03 -1.04417924e-03]

Mean Squared Error on training data (scikit-learn) with lambda = 0.1:
104.95372161311866
Cluster assignments:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
-----------------------------------
Regression weights for cluster 0: y = -1.2854x_0 + -1.6146x_1 + -1.0863x_2 + -10.0907
Regression weights for cluster 0 after refit: y = -1.2815x_1 + -1.6131x_2 + -1.0817x_3 + -10.1048
-----------------------------------
Regression weights for cluster 1: y = 0.2194x_0 + 0.1759x_1 + 0.0963x_2 + 9.8793
Regression weights for cluster 1 after refit: y = 0.2111x_1 + 0.168x_2 + 0.0942x_3 + 9.8997
-----------------------------------
Regression weights for cluster 2: y = 1.5504x_0 + 1.7399x_1 + 0.9836x_2 + 1.0004
Regression weights for cluster 2 after refit: y = 1.5505x_1 + 1.7406x_2 + 0.9834x_3 + 1.0002
{'time_milp': 190.1246178150177, 'time_greedy': np.float64(0.478764021396637), 'time_refit_milp_assignment': 192.8069305419922, 'mse_refit_ground_truth_assignment': np.float64(0.357462924773666), 'r2_refit_ground_truth_assignment': 0.99675154068014, 'weight_mismatch_refit_ground_truth_assignment': np.float64(0.49049575085896346), 'refit-weight_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'rand_score_refit_ground_truth_assignment': 1.0, 'label_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'mse_milp': np.float64(0.3575087530974632), 'r2_milp': 0.9967511242133257, 'weight_mismatch_milp': np.float64(0.4988682463882531), 'refit-weight_mismatch_milp': np.float64(0.03975982888684741), 'rand_score_milp': 1.0, 'label_mismatch_milp': np.float64(0.0), 'mse_refit_milp_assignment': np.float64(0.357462924773666), 'r2_refit_milp_assignment': 0.99675154068014, 'weight_mismatch_refit_milp_assignment': np.float64(0.4904957508589636), 'refit-weight_mismatch_refit_milp_assignment': np.float64(0.0), 'rand_score_refit_milp_assignment': 1.0, 'label_mismatch_refit_milp_assignment': np.float64(0.0), 'mse_greedy': np.float64(6.410727559693987), 'r2_greedy': np.float64(0.9417422444535845), 'weight_mismatch_greedy': np.float64(13.299816073330865), 'refit-weight_mismatch_greedy': np.float64(13.136478861694238), 'rand_score_greedy': np.float64(0.7968505477308294), 'label_mismatch_greedy': np.float64(0.25833333333333336), 'mse_greedy_sem': np.float64(1.7629808471482538), 'r2_greedy_sem': np.float64(0.01602116238286659), 'weight_mismatch_greedy_sem': np.float64(2.975982689526744), 'refit-weight_mismatch_greedy_sem': np.float64(2.9985943183028314), 'rand_score_greedy_sem': np.float64(0.036285058297240894), 'label_mismatch_greedy_sem': np.float64(0.04737248195718296), 'mse_ground_truth': np.float64(0.3556824554941374), 'r2_ground_truth': np.float64(0.9967992192488507), 'weight_mismatch_ground_truth': 0, 'rand_score_ground_truth': 1, 'label_mismatch_ground_truth': 0, 'mse_baseline_sklearn': np.float64(104.95372161311866), 'r2_baseline_sklearn': np.float64(0.04622865337994819), 'mse_milp_val': np.float64(0.3496272615005436), 'r2_milp_val': 0.9968976173751362, 'label_mismatch_milp_val': np.float64(0.0), 'mse_refit_milp_assignment_val': np.float64(0.3489489494309548), 'r2_refit_milp_assignment_val': 0.9969036363096148, 'label_mismatch_refit_milp_assignment_val': np.float64(0.0), 'mse_greedy_val': np.float64(14.172084315973436), 'label_mismatch_greedy_val': np.float64(0.2520833333333333), 'mse_greedy_val_sem': np.float64(4.596401572920048), 'label_mismatch_greedy_val_sem': np.float64(0.04452632032578468), 'r2_greedy_val': np.float64(0.8742454236798312), 'r2_greedy_val_sem': np.float64(0.04078571080390982), 'mse_refit_ground_truth_assignment_val': np.float64(0.3489489494309548), 'r2_refit_ground_truth_assignment_val': 0.9969036363096148, 'label_mismatch_refit_ground_truth_assignment_val': np.float64(0.0), 'mse_ground_truth_val': np.float64(0.33292609510969745), 'r2_ground_truth_val': 0.9970458135089374, 'label_mismatch_ground_truth_val': np.float64(0.0), 'mse_baseline_sklearn_val': np.float64(112.78653661298917), 'r2_baseline_sklearn_val': -0.0008000806486496703}

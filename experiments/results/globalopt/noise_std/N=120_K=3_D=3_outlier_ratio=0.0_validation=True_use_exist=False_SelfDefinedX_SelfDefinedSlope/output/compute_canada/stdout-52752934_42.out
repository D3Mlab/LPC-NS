==================== Evaluating with noise_std = 0.6 in Dataset 1 with random state = 2 ====================
ODS is enabled
mse 0.4042169150046978
Splitting data into training and validation set
Not whitening the data...
Set parameter TokenServer to value "license1.computecanada.ca"
Set parameter Threads to value 16
Read parameters from file gurobi.env
Set parameter MIPGap to value 0.05
Set parameter TimeLimit to value 36000
Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (linux64 - "AlmaLinux 9.3 (Shamrock Pampas Cat)")

CPU model: Intel(R) Xeon(R) Platinum 8160 CPU @ 2.10GHz, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 48 physical cores, 48 logical processors, using up to 16 threads

Optimize a model with 72 rows, 312 columns and 216 nonzeros
Model fingerprint: 0x48131544
Model has 84 quadratic objective terms
Model has 216 general constraints
Variable types: 96 continuous, 216 integer (216 binary)
Coefficient statistics:
  Matrix range     [1e+00, 1e+00]
  Objective range  [0e+00, 0e+00]
  QObjective range [2e-02, 2e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 1e+00]
  GenCon rhs range [7e-01, 2e+01]
  GenCon coe range [3e-03, 4e+00]
Presolve added 216 rows and 204 columns
Presolve time: 0.03s
Presolved: 288 rows, 516 columns, 1512 nonzeros
Presolved model has 216 SOS constraint(s)
Presolved model has 84 quadratic objective terms
Variable types: 300 continuous, 216 integer (216 binary)

Root relaxation: objective 0.000000e+00, 577 iterations, 0.01 seconds (0.00 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0    0.00000    0   72          -    0.00000      -     -    0s
H    0     0                    7900.7666867    0.00000   100%     -    0s
H    0     0                    7298.8171993    0.00000   100%     -    0s
     0     0    0.36757    0   71 7298.81720    0.36757   100%     -    0s
     0     2    0.36757    0   71 7298.81720    0.36757   100%     -    0s
H   31    48                    7269.2977184    0.36757   100%   2.4    0s
H   35    48                    6691.7474848    0.36757   100%   2.5    0s
H   79    96                    6193.4138965    0.36757   100%   2.4    0s
H  467   513                    6025.6399578    0.36757   100%   2.5    0s
H  505   513                    5680.9947139    0.36757   100%   2.5    0s
H 1600  1625                    5266.9800238    0.41871   100%   2.4    0s
H 1600  1625                    2837.9516347    0.41871   100%   2.4    0s
H 1605  1625                    2460.2880546    0.41871   100%   2.4    0s
H 1729  1961                    1613.7553860    0.41871   100%   2.4    0s
H 2360  2531                     979.2940499    0.41871   100%   2.4    0s
H 2361  2531                     786.0276220    0.41871   100%   2.4    0s
H 2499  2481                     470.4584861    0.41871   100%   2.4    0s
H 3739  3313                     369.1744856    0.41871   100%   2.2    0s
H 3867  3261                     345.7161861    0.41871   100%   2.2    0s
H 3960  3151                     302.4019083    0.41871   100%   2.2    0s
H 6298  3962                     260.7391142    0.41871   100%   2.2    0s
H 6300  3763                     257.6117096    0.41871   100%   2.2    0s
H 6302  3576                     253.6953430    0.41871   100%   2.2    0s
H 6383  3443                     253.6102171    0.41871   100%   2.3    0s
H 6385  3272                     237.1317937    0.41871   100%   2.3    0s
H 6451  3135                     225.0182921    0.41871   100%   2.3    0s
H 6602  3085                     216.9980544    0.41871   100%   2.3    1s
H 7641  3323                     172.5705661    0.41871   100%   2.3    1s
H 7643  3183                     171.4462740    0.41871   100%   2.3    1s
H 7754  3126                     170.8648275    0.41871   100%   2.3    1s
H 7850  3172                     170.5337714    0.41871   100%   2.3    1s
H 7909  3033                     170.2934589    0.41871   100%   2.3    1s
H15279  5269                     170.2600917    0.47210   100%   2.2    1s
H15403  5266                     170.1404425    0.47210   100%   2.2    1s
H27397 10638                     169.8234431    0.59821   100%   2.1    1s
 109717 53528   16.00779   67   68  169.82344    0.91440  99.5%   2.0    5s
*132980 61984             133     146.9074605    0.98824  99.3%   2.0    5s
H135979 63062                     146.2952939    0.99222  99.3%   2.0    5s
H135981 62913                     145.0049021    0.99222  99.3%   2.0    5s
H135987 56687                      96.4235133    0.99222  99.0%   2.0    5s
H138007 58049                      96.3755577    0.99222  99.0%   2.0    6s
H138008 54026                      73.3228883    0.99222  98.6%   2.0    6s
H138012 53066                      67.4062721    0.99222  98.5%   2.0    6s
H139626 52939                      60.5470531    1.00425  98.3%   2.0    6s
H139629 52428                      58.0488990    1.00425  98.3%   2.0    6s
H139632 51923                      55.4323318    1.00425  98.2%   2.0    6s
H139642 51793                      54.7405003    1.00425  98.2%   2.0    6s
H177411 66415                      50.8673077    1.18306  97.7%   2.0    7s
H177412 53682                      24.6232218    1.18306  95.2%   2.0    7s
 316324 115493     cutoff   84        24.62322    1.65161  93.3%   2.0   10s
H410692 146392                      23.4012371    1.98169  91.5%   1.9   12s
 543703 188457   21.45033   93   58   23.40124    2.38648  89.8%   1.9   15s
 789627 252702     cutoff   75        23.40124    3.02528  87.1%   1.9   20s
 1030544 309930    9.62368   99   46   23.40124    3.56341  84.8%   1.8   25s
 1278410 365632     cutoff   73        23.40124    4.04671  82.7%   1.8   30s
 1506139 406803    6.89269   82   56   23.40124    4.48792  80.8%   1.8   35s
 1725927 440754   15.09028   80   65   23.40124    4.90092  79.1%   1.8   40s
 1938527 470326     cutoff   92        23.40124    5.26589  77.5%   1.8   45s
 2150379 501346   15.63231   93   55   23.40124    5.59838  76.1%   1.8   50s
 2366290 528200     cutoff  100        23.40124    5.91873  74.7%   1.8   55s
 2582494 553536     cutoff   81        23.40124    6.21451  73.4%   1.8   60s
 2792247 572003     cutoff  102        23.40124    6.51269  72.2%   1.7   65s
 3008485 590086   21.23842   93   54   23.40124    6.83337  70.8%   1.7   70s
 3220203 604263   19.50157   94   53   23.40124    7.15780  69.4%   1.7   75s
 3430935 612862     cutoff  109        23.40124    7.50728  67.9%   1.7   80s
 3646795 620257   19.79615  103   46   23.40124    7.87082  66.4%   1.7   85s
 3859274 620445   20.67684  105   44   23.40124    8.27827  64.6%   1.7   90s
 4076658 623463     cutoff   90        23.40124    8.69422  62.8%   1.7   95s
 4297484 623709     cutoff   97        23.40124    9.13746  61.0%   1.7  100s
 4516616 624286   14.32668   97   54   23.40124    9.57903  59.1%   1.7  105s
 4735129 621249   21.30479  105   44   23.40124   10.04955  57.1%   1.7  110s
 4941359 616841   15.47241  106   44   23.40124   10.51659  55.1%   1.7  115s
 5164024 612218     cutoff  103        23.40124   11.03551  52.8%   1.7  120s
 5378055 605732   20.27157  103   47   23.40124   11.55958  50.6%   1.7  125s
 5593065 597696   14.58749   98   53   23.40124   12.09178  48.3%   1.7  130s
 5803748 584821     cutoff   96        23.40124   12.63704  46.0%   1.6  135s
 6026362 569598   14.19372  102   48   23.40124   13.22476  43.5%   1.6  140s
 6237052 553089   13.80253  109   43   23.40124   13.78666  41.1%   1.6  145s
 6454871 531816   18.55584   87   59   23.40124   14.41154  38.4%   1.6  150s
 6670523 509698   15.03964   78   66   23.40124   15.03704  35.7%   1.6  155s
 6887282 486233     cutoff   90        23.40124   15.66407  33.1%   1.6  160s
 7100543 458605     cutoff   91        23.40124   16.29161  30.4%   1.6  165s
 7314953 427374   20.97828   96   51   23.40124   16.93387  27.6%   1.6  170s
 7532928 392034     cutoff   88        23.40124   17.62718  24.7%   1.6  175s
 7753882 349651   20.63398  100   50   23.40124   18.35196  21.6%   1.6  180s
 7969797 307130   21.37331   75   66   23.40124   19.05628  18.6%   1.6  185s
 8191674 256417     cutoff  100        23.40124   19.81971  15.3%   1.6  190s
 8408618 195396   21.95571  100   49   23.40124   20.64384  11.8%   1.6  195s
 8636032 122416     cutoff   97        23.40124   21.57043  7.82%   1.6  200s

Explored 8771258 nodes (13876223 simplex iterations) in 203.34 seconds (150.70 work units)
Thread count was 16 (of 48 available processors)

Solution count 10: 23.4012 24.6232 50.8673 ... 96.3756

Optimal solution found (tolerance 5.00e-02)
Best objective 2.340123711018e+01, best bound 2.224198331589e+01, gap 4.9538%
Optimal solution found.
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]
Cluster Assignments from Gurobi MIQP using MSE

Cluster 0 weights w_0^* (including bias) in original space:
[-10.31084792  -1.26193252  -1.52226353  -1.07085532]

Cluster 1 weights w_1^* (including bias) in original space:
[1.15164844 1.49162797 1.73991803 0.87106525]

Cluster 2 weights w_2^* (including bias) in original space:
[10.11871312  0.14505702  0.02187631 -0.04481213]
############################################################################

Ridge Regression using scikit-learn:
Optimal weights w* in X Space (including bias):
[ 6.44981449e-01 -4.73521731e-06  4.04315453e-04  2.36534728e-04]

Mean Squared Error on training data (scikit-learn) with lambda = 0.1:
109.73280269195465
Cluster assignments:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]
-----------------------------------
Regression weights for cluster 0: y = -1.2619x_0 + -1.5223x_1 + -1.0709x_2 + -10.3108
Regression weights for cluster 0 after refit: y = -1.2569x_1 + -1.519x_2 + -1.0657x_3 + -10.3273
-----------------------------------
Regression weights for cluster 1: y = 1.4916x_0 + 1.7399x_1 + 0.8711x_2 + 1.1516
Regression weights for cluster 1 after refit: y = 1.4918x_1 + 1.7405x_2 + 0.8707x_3 + 1.1519
-----------------------------------
Regression weights for cluster 2: y = 0.1451x_0 + 0.0219x_1 + -0.0448x_2 + 10.1187
Regression weights for cluster 2 after refit: y = 0.1426x_1 + 0.0166x_2 + -0.0474x_3 + 10.1324
{'time_milp': 203.93696236610413, 'time_greedy': np.float64(0.5306669354438782), 'time_refit_milp_assignment': 207.00177812576294, 'mse_refit_ground_truth_assignment': np.float64(0.29426560897451254), 'r2_refit_ground_truth_assignment': 0.9973287352617198, 'weight_mismatch_refit_ground_truth_assignment': np.float64(0.8955246631097485), 'refit-weight_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'rand_score_refit_ground_truth_assignment': 1.0, 'label_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'mse_milp': np.float64(0.2943061713883993), 'r2_milp': 0.9973283670469417, 'weight_mismatch_milp': np.float64(0.8667446530219285), 'refit-weight_mismatch_milp': np.float64(0.034124593556189284), 'rand_score_milp': 1.0, 'label_mismatch_milp': np.float64(0.0), 'mse_refit_milp_assignment': np.float64(0.29426560897451254), 'r2_refit_milp_assignment': 0.9973287352617198, 'weight_mismatch_refit_milp_assignment': np.float64(0.8955246631097485), 'refit-weight_mismatch_refit_milp_assignment': np.float64(0.0), 'rand_score_refit_milp_assignment': 1.0, 'label_mismatch_refit_milp_assignment': np.float64(0.0), 'mse_greedy': np.float64(4.984648498414901), 'r2_greedy': np.float64(0.9547506899873908), 'weight_mismatch_greedy': np.float64(11.032250783737148), 'refit-weight_mismatch_greedy': np.float64(10.735933174392974), 'rand_score_greedy': np.float64(0.7929577464788733), 'label_mismatch_greedy': np.float64(0.25347222222222227), 'mse_greedy_sem': np.float64(1.6937303533025918), 'r2_greedy_sem': np.float64(0.01537523254823808), 'weight_mismatch_greedy_sem': np.float64(2.6165960411634543), 'refit-weight_mismatch_greedy_sem': np.float64(2.6713995041384226), 'rand_score_greedy_sem': np.float64(0.027964641062755966), 'label_mismatch_greedy_sem': np.float64(0.039795649938920194), 'mse_ground_truth': np.float64(0.4042169150046978), 'r2_ground_truth': np.float64(0.9964108009325497), 'weight_mismatch_ground_truth': 0, 'rand_score_ground_truth': 1, 'label_mismatch_ground_truth': 0, 'mse_baseline_sklearn': np.float64(109.73280269195465), 'r2_baseline_sklearn': np.float64(0.0038748752013955112), 'mse_milp_val': np.float64(0.8713150703162262), 'r2_milp_val': 0.9925086747986273, 'label_mismatch_milp_val': np.float64(0.041666666666666664), 'mse_refit_milp_assignment_val': np.float64(0.8739714749628341), 'r2_refit_milp_assignment_val': 0.9924858357685771, 'label_mismatch_refit_milp_assignment_val': np.float64(0.041666666666666664), 'mse_greedy_val': np.float64(12.054035995776289), 'label_mismatch_greedy_val': np.float64(0.2427083333333334), 'mse_greedy_val_sem': np.float64(4.20859113948206), 'label_mismatch_greedy_val_sem': np.float64(0.039362698811800725), 'r2_greedy_val': np.float64(0.8963627432719141), 'r2_greedy_val_sem': np.float64(0.03618429881401397), 'mse_refit_ground_truth_assignment_val': np.float64(0.8739714749628341), 'r2_refit_ground_truth_assignment_val': 0.9924858357685771, 'label_mismatch_refit_ground_truth_assignment_val': np.float64(0.041666666666666664), 'mse_ground_truth_val': np.float64(0.8593093750601755), 'r2_ground_truth_val': 0.9926118964350888, 'label_mismatch_ground_truth_val': np.float64(0.041666666666666664), 'mse_baseline_sklearn_val': np.float64(116.31120294754824), 'r2_baseline_sklearn_val': -1.1448816920500704e-05}

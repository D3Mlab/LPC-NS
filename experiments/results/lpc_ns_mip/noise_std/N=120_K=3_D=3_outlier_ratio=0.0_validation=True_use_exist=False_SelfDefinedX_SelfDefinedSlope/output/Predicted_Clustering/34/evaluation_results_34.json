{"time_milp": 4312.108739852905, "time_greedy": 0.4636399507522583, "time_refit_milp_assignment": 4314.810897111893, "mse_refit_ground_truth_assignment": 1.0100350186804503, "r2_refit_ground_truth_assignment": 0.9909797244814463, "weight_mismatch_refit_ground_truth_assignment": 2.0044288801308077, "refit-weight_mismatch_refit_ground_truth_assignment": 0.0, "rand_score_refit_ground_truth_assignment": 1.0, "label_mismatch_refit_ground_truth_assignment": 0.0, "mse_milp": 4.701543153050941, "r2_milp": 0.9580121344126327, "weight_mismatch_milp": 7.357448447369841, "refit-weight_mismatch_milp": 8.39006241287806, "rand_score_milp": 0.8110328638497653, "label_mismatch_milp": 0.16666666666666666, "mse_refit_milp_assignment": 4.70149276240473, "r2_refit_milp_assignment": 0.9580125844341705, "weight_mismatch_refit_milp_assignment": 7.34562978167999, "refit-weight_mismatch_refit_milp_assignment": 8.418900044747232, "rand_score_refit_milp_assignment": 0.8110328638497653, "label_mismatch_refit_milp_assignment": 0.16666666666666666, "mse_greedy": 3.4675583805665093, "r2_greedy": 0.9690324281921999, "weight_mismatch_greedy": 11.114746259857668, "refit-weight_mismatch_greedy": 10.576945254525194, "rand_score_greedy": 0.8172535211267606, "label_mismatch_greedy": 0.19930555555555554, "mse_greedy_sem": 0.6376810150903046, "r2_greedy_sem": 0.005694909921618572, "weight_mismatch_greedy_sem": 2.221190057883414, "refit-weight_mismatch_greedy_sem": 2.4072421381267075, "rand_score_greedy_sem": 0.020711906474818166, "label_mismatch_greedy_sem": 0.027191827971783055, "mse_ground_truth": 1.124112703582176, "r2_ground_truth": 0.9899657094231166, "weight_mismatch_ground_truth": 0, "rand_score_ground_truth": 1, "label_mismatch_ground_truth": 0, "mse_baseline_sklearn": 104.8379816034512, "r2_baseline_sklearn": 0.06372802785823839, "mse_milp_val": 8.08248889293519, "r2_milp_val": 0.9277677696419386, "label_mismatch_milp_val": 0.1875, "mse_refit_milp_assignment_val": 8.084853976899288, "r2_refit_milp_assignment_val": 0.9277466331712335, "label_mismatch_refit_milp_assignment_val": 0.1875, "mse_greedy_val": 8.623691544884053, "label_mismatch_greedy_val": 0.19583333333333336, "mse_greedy_val_sem": 2.101743824628621, "label_mismatch_greedy_val_sem": 0.029863236018507374, "r2_greedy_val": 0.9229311066852925, "r2_greedy_val_sem": 0.018783031576684987, "mse_refit_ground_truth_assignment_val": 1.172112520288987, "r2_refit_ground_truth_assignment_val": 0.9895249714917534, "label_mismatch_refit_ground_truth_assignment_val": 0.020833333333333332, "mse_ground_truth_val": 0.9572708107655057, "r2_ground_truth_val": 0.991444985989563, "label_mismatch_ground_truth_val": 0.020833333333333332, "mse_baseline_sklearn_val": 112.25668750079501, "r2_baseline_sklearn_val": -0.003224504011124063}
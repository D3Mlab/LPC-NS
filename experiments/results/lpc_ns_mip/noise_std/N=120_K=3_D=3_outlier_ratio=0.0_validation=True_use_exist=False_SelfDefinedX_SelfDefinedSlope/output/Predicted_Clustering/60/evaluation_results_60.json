{"time_milp": 1939.4707753658295, "time_greedy": 0.4746772289276123, "time_refit_milp_assignment": 1942.2152469158173, "mse_refit_ground_truth_assignment": 7.967024108919448, "r2_refit_ground_truth_assignment": 0.9382268991659877, "weight_mismatch_refit_ground_truth_assignment": 5.1022556991067844, "refit-weight_mismatch_refit_ground_truth_assignment": 0.0, "rand_score_refit_ground_truth_assignment": 1.0, "label_mismatch_refit_ground_truth_assignment": 0.0, "mse_milp": 6.472047552383942, "r2_milp": 0.9498183461490541, "weight_mismatch_milp": 5.322233651792007, "refit-weight_mismatch_milp": 5.788188285693263, "rand_score_milp": 0.7953834115805947, "label_mismatch_milp": 0.20833333333333334, "mse_refit_milp_assignment": 6.472003069659373, "r2_refit_milp_assignment": 0.9498186910502109, "weight_mismatch_refit_milp_assignment": 5.324658805747203, "refit-weight_mismatch_refit_milp_assignment": 5.760100807344811, "rand_score_refit_milp_assignment": 0.7953834115805947, "label_mismatch_refit_milp_assignment": 0.20833333333333334, "mse_greedy": 14.200101802055453, "r2_greedy": 0.8898981214968883, "weight_mismatch_greedy": 22.16231123762348, "refit-weight_mismatch_greedy": 22.03187479392088, "rand_score_greedy": 0.6892410015649453, "label_mismatch_greedy": 0.3979166666666666, "mse_greedy_sem": 1.8510025819196179, "r2_greedy_sem": 0.014351929600530057, "weight_mismatch_greedy_sem": 2.3654267504312965, "refit-weight_mismatch_greedy_sem": 2.5374975850362986, "rand_score_greedy_sem": 0.016642642988365163, "label_mismatch_greedy_sem": 0.025155361150396703, "mse_ground_truth": 8.217809006156392, "r2_ground_truth": 0.9317656064770126, "weight_mismatch_ground_truth": 0, "rand_score_ground_truth": 1, "label_mismatch_ground_truth": 0, "mse_baseline_sklearn": 119.57109697584694, "r2_baseline_sklearn": 0.0728938020843718, "mse_milp_val": 15.248095545117117, "r2_milp_val": 0.8578032060412013, "label_mismatch_milp_val": 0.0625, "mse_refit_milp_assignment_val": 15.25304227816801, "r2_refit_milp_assignment_val": 0.8577570750618717, "label_mismatch_refit_milp_assignment_val": 0.0625, "mse_greedy_val": 37.489825998556725, "label_mismatch_greedy_val": 0.3572916666666666, "mse_greedy_val_sem": 5.219707036190006, "label_mismatch_greedy_val_sem": 0.03127831319719764, "r2_greedy_val": 0.650386958338866, "r2_greedy_val_sem": 0.04867661038417903, "mse_refit_ground_truth_assignment_val": 10.396152641982633, "r2_refit_ground_truth_assignment_val": 0.9030502156271168, "label_mismatch_refit_ground_truth_assignment_val": 0.1875, "mse_ground_truth_val": 10.768742144179479, "r2_ground_truth_val": 0.8995756156340664, "label_mismatch_ground_truth_val": 0.1875, "mse_baseline_sklearn_val": 107.9072987068288, "r2_baseline_sklearn_val": -0.006294318884898065}
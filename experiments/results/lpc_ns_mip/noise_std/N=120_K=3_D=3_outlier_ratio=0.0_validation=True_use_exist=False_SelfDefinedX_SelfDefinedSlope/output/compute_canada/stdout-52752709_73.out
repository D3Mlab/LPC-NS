==================== Evaluating with noise_std = 0.9 in Dataset 1 with random state = 7 ====================
ODS is enabled
mse 0.806788238295377
Splitting data into training and validation set
Not whitening the data...
Set parameter TokenServer to value "license1.computecanada.ca"
Set parameter Threads to value 16
Read parameters from file gurobi.env
Set parameter MIPGap to value 0.05
Set parameter TimeLimit to value 36000
using MSE:
Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (linux64 - "AlmaLinux 9.3 (Shamrock Pampas Cat)")

CPU model: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 48 physical cores, 48 logical processors, using up to 16 threads

Optimize a model with 72 rows, 288 columns and 216 nonzeros
Model fingerprint: 0x9adc4f71
Model has 72 quadratic objective terms
Model has 216 general constraints
Variable types: 72 continuous, 216 integer (216 binary)
Coefficient statistics:
  Matrix range     [1e+00, 1e+00]
  Objective range  [0e+00, 0e+00]
  QObjective range [2e+00, 2e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 1e+00]
  GenCon rhs range [7e-01, 2e+01]
  GenCon coe range [1e-05, 1e+01]
Presolve added 216 rows and 216 columns
Presolve time: 0.02s
Presolved: 288 rows, 504 columns, 16200 nonzeros
Presolved model has 216 SOS constraint(s)
Presolved model has 72 quadratic objective terms
Variable types: 288 continuous, 216 integer (216 binary)
Found heuristic solution: objective 8837.4553698
Found heuristic solution: objective 7078.6873387

Root relaxation: objective 0.000000e+00, 671 iterations, 0.01 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0    0.00000    0   83 7078.68734    0.00000   100%     -    0s
H    0     0                    5775.6591495    0.00000   100%     -    0s
H    0     0                    5644.4533163    0.00000   100%     -    0s
H    0     0                    4408.4551347    0.00000   100%     -    0s
     0     0    0.00000    0   82 4408.45513    0.00000   100%     -    0s
H    0     0                    3672.2273584    0.00000   100%     -    0s
     0     2    0.00000    0   82 3672.22736    0.00000   100%     -    0s
H   35    48                    1068.3426998    0.00000   100%  28.4    0s
H  642   655                     936.2470925    0.00000   100%  22.8    0s
H  655   671                     860.1193182    0.00000   100%  22.6    1s
H  656   671                     624.2577932    0.00000   100%  22.6    1s
H  663   671                     576.4492388    0.00000   100%  22.5    1s
H 2808  2155                     452.4770571    0.00000   100%  24.4    2s
H 2813  2053                     432.4156436    0.00000   100%  24.5    2s
  6083  2570  389.45566   27   82  432.41564    1.76801   100%  28.4    6s
H 6954  2510                     417.0508333    1.80052   100%  28.6    6s
H 6990  2389                     404.6267032    1.80052   100%  28.6    6s
H 7004  2216                     374.4982042    1.80052   100%  28.6    6s
H 9470  2970                     338.7296680    2.36127  99.3%  26.7    6s
 44631 14743  188.94275   47   73  338.72967   24.09851  92.9%  20.6   10s
 104035 32922  147.92358   35   79  338.72967   57.50925  83.0%  18.3   15s
*126741 38478             121     317.6527556   65.88826  79.3%  18.0   16s
 164793 48309     cutoff   44       317.65276   78.38010  75.3%  17.2   20s
 233391 67096     cutoff   31       317.65276   95.97662  69.8%  16.2   25s
*242454 69344             116     315.7652086   97.58960  69.1%  16.0   25s
 269788 77587     cutoff   46       315.76521  102.68737  67.5%  15.6   30s
*295711 83876             123     307.0396607  106.35980  65.4%  15.2   31s
 340180 98440  208.87904   48   73  307.03966  112.10500  63.5%  14.6   35s
H342081 97304                     299.9987381  112.33034  62.6%  14.5   35s
H342125 96882                     298.4702817  112.33034  62.4%  14.5   35s
*349363 94790             110     286.1710326  113.13651  60.5%  14.4   35s
*356803 94434             114     279.7451842  113.95762  59.3%  14.3   36s
 401270 107520     cutoff   78       279.74518  118.70764  57.6%  13.8   40s
 474488 128351  228.74311  101   23  279.74518  125.30233  55.2%  13.0   45s
H487476 119043                     260.1539422  126.26945  51.5%  12.9   46s
 543171 133972  183.69523   78   48  260.15394  130.40753  49.9%  12.4   50s
 616269 152953  211.36221   87   42  260.15394  134.92536  48.1%  11.9   55s
H676854 168244                     259.6287189  137.77481  46.9%  11.5   59s
H676861 166801                     257.8586420  137.77738  46.6%  11.5   59s
 686033 169206     cutoff   93       257.85864  138.21843  46.4%  11.4   61s
 737219 182407  227.02580   81   35  257.85864  140.25792  45.6%  11.1   65s
 779743 193182  232.90745   91   33  257.85864  141.75860  45.0%  10.9   70s
H780908 184892                     249.1892879  141.75860  43.1%  10.9   70s
 849793 200915     cutoff   73       249.18929  144.14117  42.2%  10.6   75s
 922428 213752  217.83393   88   32  249.18929  146.76726  41.1%  10.4   80s
 979185 222429     cutoff   90       249.18929  148.79091  40.3%  10.2   85s
 1044579 231238     cutoff   79       249.18929  151.24306  39.3%  10.1   90s
 1100281 238800  240.86996   78   42  249.18929  153.23681  38.5%   9.9   95s
 1122769 241666  245.70818   54   50  249.18929  154.03395  38.2%   9.9  100s
 1185206 249098     cutoff   32       249.18929  156.29127  37.3%   9.8  105s
 1258402 257254     cutoff   83       249.18929  158.95220  36.2%   9.7  110s
 1324158 264391     cutoff   87       249.18929  161.25909  35.3%   9.6  115s
 1377206 269637     cutoff   63       249.18929  163.12833  34.5%   9.5  120s
 1448709 276379     cutoff   83       249.18929  165.57596  33.6%   9.4  125s
 1493008 280091     cutoff   78       249.18929  167.00320  33.0%   9.3  130s
 1564864 285500  194.36869  100   22  249.18929  169.32637  32.0%   9.2  135s
 1637643 290088     cutoff   74       249.18929  171.61046  31.1%   9.1  140s
 1679165 292568     cutoff   49       249.18929  172.83107  30.6%   9.1  145s
 1750269 296399     cutoff   75       249.18929  174.88968  29.8%   9.0  150s
 1810366 299114     cutoff   87       249.18929  176.56375  29.1%   9.0  157s
 1851123 300395  217.77371   85   41  249.18929  177.71489  28.7%   8.9  160s
 1923203 301961  228.84628   81   40  249.18929  179.65954  27.9%   8.9  165s
 1993089 303258     cutoff   89       249.18929  181.50231  27.2%   8.8  170s
 2040870 303560  218.78626   99   28  249.18929  182.67743  26.7%   8.8  175s
 2098162 303492  197.01021   67   58  249.18929  184.11560  26.1%   8.7  180s
 2142285 302969  236.24526   84   39  249.18929  185.24430  25.7%   8.7  186s
 2198423 302053     cutoff   83       249.18929  186.61872  25.1%   8.6  190s
 2267687 300078     cutoff   84       249.18929  188.33072  24.4%   8.6  195s
 2323553 297814  216.83318   80   44  249.18929  189.74326  23.9%   8.6  200s
 2386349 294892     cutoff   75       249.18929  191.26700  23.2%   8.5  205s
 2406264 293860     cutoff   78       249.18929  191.76135  23.0%   8.5  211s
 2456409 290045     cutoff   85       249.18929  193.04115  22.5%   8.5  215s
 2528839 283901  206.45828   70   51  249.18929  194.88983  21.8%   8.4  220s
 2592232 277930  232.92233   96   32  249.18929  196.49831  21.1%   8.4  226s
 2649003 271292     cutoff   88       249.18929  197.97759  20.6%   8.4  230s
 2697388 265129  247.93749   91   28  249.18929  199.26967  20.0%   8.4  235s
 2768107 255433  238.46198   72   40  249.18929  201.14177  19.3%   8.3  240s
 2834305 245059  222.67390   81   40  249.18929  202.94332  18.6%   8.3  245s
 2887398 235675     cutoff   84       249.18929  204.41395  18.0%   8.3  250s
 2951652 223175  241.53234   62   58  249.18929  206.28275  17.2%   8.2  255s
 3015762 208410     cutoff   87       249.18929  208.34009  16.4%   8.2  260s
 3061440 196861     cutoff   88       249.18929  209.83633  15.8%   8.2  265s
 3132538 177731     cutoff   89       249.18929  212.31756  14.8%   8.2  270s
 3152274 171800     cutoff   92       249.18929  213.00822  14.5%   8.1  275s
 3222974 149110     cutoff   93       249.18929  215.89523  13.4%   8.1  280s
 3296857 122284     cutoff   73       249.18929  219.36416  12.0%   8.1  285s
 3347870 100483  236.80424   78   32  249.18929  222.35755  10.8%   8.0  290s
 3419898 65890  235.31292   97   23  249.18929  227.92073  8.54%   8.0  295s
 3433464 58667     cutoff   79       249.18929  229.27839  7.99%   8.0  300s

Explored 3481306 nodes (27657899 simplex iterations) in 303.14 seconds (319.52 work units)
Thread count was 16 (of 48 available processors)

Solution count 10: 249.189 257.859 259.629 ... 315.765

Optimal solution found (tolerance 5.00e-02)
Best objective 2.491892879032e+02, best bound 2.368152393600e+02, gap 4.9657%

Gurobi MIQP converged to an optimal solution.
Cluster Assignments from Gurobi MIQP using MSE
############################################################################

Ridge Regression using scikit-learn:
Optimal weights w* in X Space (including bias):
[ 4.02619555e-01  9.98391781e-05 -6.43511028e-04  3.85528592e-04]

Mean Squared Error on training data (scikit-learn) with lambda = 0.1:
117.79724298157605
Cluster assignments:  [0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 2 2 2 1 2 2 2 1 2 2
 2 1 2 2 1 2 2 2 1 2 2 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 2 1 1 1]
-----------------------------------
Regression weights for cluster 0: y = -1.6482x_0 + -1.8703x_1 + -0.9556x_2 + -9.7533
Regression weights for cluster 0 after refit: y = -1.6432x_1 + -1.8671x_2 + -0.9502x_3 + -9.7718
-----------------------------------
Regression weights for cluster 1: y = -0.4801x_0 + 1.3613x_1 + -0.5457x_2 + 8.8454
Regression weights for cluster 1 after refit: y = -0.4859x_1 + 1.3544x_2 + -0.5488x_3 + 8.8652
-----------------------------------
Regression weights for cluster 2: y = 1.4309x_0 + -2.3528x_1 + 3.6534x_2 + 2.3868
Regression weights for cluster 2 after refit: y = 1.4309x_1 + -2.355x_2 + 3.6538x_3 + 2.3895
{'time_milp': 303.7284083366394, 'time_greedy': np.float64(0.48223448991775514), 'time_refit_milp_assignment': 306.4601728916168, 'mse_refit_ground_truth_assignment': np.float64(0.560888149669305), 'r2_refit_ground_truth_assignment': 0.9952816458487453, 'weight_mismatch_refit_ground_truth_assignment': np.float64(0.9937282497696122), 'refit-weight_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'rand_score_refit_ground_truth_assignment': 1.0, 'label_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'mse_milp': np.float64(2.864832028508874), 'r2_milp': 0.9759002002407581, 'weight_mismatch_milp': np.float64(7.298986385747685), 'refit-weight_mismatch_milp': np.float64(6.780650601286959), 'rand_score_milp': np.float64(0.8039906103286385), 'label_mismatch_milp': np.float64(0.19444444444444445), 'mse_refit_milp_assignment': np.float64(2.864784120819028), 'r2_refit_milp_assignment': 0.9759006032541706, 'weight_mismatch_refit_milp_assignment': np.float64(7.274955303389342), 'refit-weight_mismatch_refit_milp_assignment': np.float64(6.779032162945898), 'rand_score_refit_milp_assignment': np.float64(0.8039906103286385), 'label_mismatch_refit_milp_assignment': np.float64(0.19444444444444445), 'mse_greedy': np.float64(5.769713611046529), 'r2_greedy': np.float64(0.9514634919916165), 'weight_mismatch_greedy': np.float64(14.723490857780357), 'refit-weight_mismatch_greedy': np.float64(14.415793550558394), 'rand_score_greedy': np.float64(0.8082942097026604), 'label_mismatch_greedy': np.float64(0.22777777777777777), 'mse_greedy_sem': np.float64(1.6704495312359187), 'r2_greedy_sem': np.float64(0.014052307014892955), 'weight_mismatch_greedy_sem': np.float64(3.2651063655744252), 'refit-weight_mismatch_greedy_sem': np.float64(3.3103804657685814), 'rand_score_greedy_sem': np.float64(0.029509560563579097), 'label_mismatch_greedy_sem': np.float64(0.040647876175230295), 'mse_ground_truth': np.float64(0.806788238295377), 'r2_ground_truth': np.float64(0.9929363031132942), 'weight_mismatch_ground_truth': 0, 'rand_score_ground_truth': 1, 'label_mismatch_ground_truth': 0, 'mse_baseline_sklearn': np.float64(117.79724298157605), 'r2_baseline_sklearn': np.float64(0.009055351309941284), 'mse_milp_val': np.float64(8.418633501734684), 'r2_milp_val': 0.9212412312781589, 'label_mismatch_milp_val': np.float64(0.16666666666666666), 'mse_refit_milp_assignment_val': np.float64(8.420830630968634), 'r2_refit_milp_assignment_val': 0.9212206764941607, 'label_mismatch_refit_milp_assignment_val': np.float64(0.16666666666666666), 'mse_greedy_val': np.float64(17.361634959738296), 'label_mismatch_greedy_val': np.float64(0.22604166666666664), 'mse_greedy_val_sem': np.float64(5.998319447674206), 'label_mismatch_greedy_val_sem': np.float64(0.03270637774911548), 'r2_greedy_val': np.float64(0.8375768475792057), 'r2_greedy_val_sem': np.float64(0.056116025718633625), 'mse_refit_ground_truth_assignment_val': np.float64(1.2246382026681406), 'r2_refit_ground_truth_assignment_val': 0.9885431528819971, 'label_mismatch_refit_ground_truth_assignment_val': np.float64(0.041666666666666664), 'mse_ground_truth_val': np.float64(1.1733574429276123), 'r2_ground_truth_val': 0.989022899327243, 'label_mismatch_ground_truth_val': np.float64(0.041666666666666664), 'mse_baseline_sklearn_val': np.float64(107.4592599475075), 'r2_baseline_sklearn_val': -0.005312679246820995}

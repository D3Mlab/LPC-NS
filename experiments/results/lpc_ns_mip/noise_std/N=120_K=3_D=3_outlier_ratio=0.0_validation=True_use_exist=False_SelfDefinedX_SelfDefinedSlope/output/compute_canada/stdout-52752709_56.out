==================== Evaluating with noise_std = 1.7 in Dataset 1 with random state = 5 ====================
ODS is enabled
mse 2.823955770248749
Splitting data into training and validation set
Not whitening the data...
Set parameter TokenServer to value "license1.computecanada.ca"
Set parameter Threads to value 16
Read parameters from file gurobi.env
Set parameter MIPGap to value 0.05
Set parameter TimeLimit to value 36000
using MSE:
Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (linux64 - "AlmaLinux 9.3 (Shamrock Pampas Cat)")

CPU model: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 48 physical cores, 48 logical processors, using up to 16 threads

Optimize a model with 72 rows, 288 columns and 216 nonzeros
Model fingerprint: 0x1deee878
Model has 72 quadratic objective terms
Model has 216 general constraints
Variable types: 72 continuous, 216 integer (216 binary)
Coefficient statistics:
  Matrix range     [1e+00, 1e+00]
  Objective range  [0e+00, 0e+00]
  QObjective range [2e+00, 2e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 1e+00]
  GenCon rhs range [7e-01, 2e+01]
  GenCon coe range [3e-05, 9e+00]
Presolve added 216 rows and 216 columns
Presolve time: 0.06s
Presolved: 288 rows, 504 columns, 16200 nonzeros
Presolved model has 216 SOS constraint(s)
Presolved model has 72 quadratic objective terms
Variable types: 288 continuous, 216 integer (216 binary)
Found heuristic solution: objective 10894.769933
Found heuristic solution: objective 7890.3395973

Root relaxation: objective 0.000000e+00, 659 iterations, 0.01 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0    0.00000    0   72 7890.33960    0.00000   100%     -    0s
H    0     0                    5445.4237483    0.00000   100%     -    0s
     0     0    0.00000    0   74 5445.42375    0.00000   100%     -    0s
H    0     0                    4815.9390808    0.00000   100%     -    0s
     0     2    0.00000    0   74 4815.93908    0.00000   100%     -    0s
H   31    48                    3774.4102792    0.00000   100%  38.5    0s
H   35    48                    3514.0159861    0.00000   100%  34.5    0s
H  138   158                    3236.9832636    0.00000   100%  37.8    0s
H  195   224                    2787.4003737    0.00000   100%  41.6    0s
H  200   224                    2328.8190045    0.00000   100%  42.2    0s
H  622   639                    2241.1978555    0.00000   100%  42.5    1s
H  627   639                    1945.1546914    0.00000   100%  42.3    1s
H  632   639                    1689.4281834    0.00000   100%  42.2    1s
H  633   639                    1111.6131640    0.00000   100%  42.2    1s
H  641   655                    1042.7008419    0.00000   100%  42.0    1s
H 2927  2118                     994.6390949    0.00000   100%  34.6    3s
H 3947  2483                     749.1660458    0.00000   100%  33.7    4s
 11288  4483  660.79009   32   69  749.16605   42.00623  94.4%  30.1    5s
 63691 24358     cutoff   28       749.16605  133.61952  82.2%  25.9   10s
*79664 29047             123     705.9101492  144.79120  79.5%  25.1   11s
 123956 43681  638.26274   38   70  705.91015  173.95973  75.4%  23.7   15s
H130629 43478                     666.7258784  177.35344  73.4%  23.5   15s
H137852 45093                     661.7825879  181.72356  72.5%  23.4   18s
 158577 52677  600.26386   32   78  661.78259  190.24067  71.3%  22.8   20s
 202835 66134     cutoff   32       661.78259  206.79630  68.8%  21.8   26s
H202837 55741                     586.9837864  206.79630  64.8%  21.8   26s
H203056 55610                     585.8002300  207.14683  64.6%  21.8   26s
 239599 65462     cutoff   28       585.80023  219.79185  62.5%  21.1   30s
 298517 81883     cutoff   38       585.80023  234.47961  60.0%  20.0   35s
H327918 85537                     565.8908527  240.24770  57.5%  19.6   39s
 338268 88401     cutoff   44       565.89085  242.75674  57.1%  19.4   40s
 403764 104425  452.72901   37   65  565.89085  255.09984  54.9%  18.6   45s
 467478 119912     cutoff   68       565.89085  265.71928  53.0%  18.1   50s
 525282 132346  406.04835   46   59  565.89085  274.60914  51.5%  17.6   55s
H525976 130602                     558.9009129  274.63969  50.9%  17.6   55s
 580412 142496  492.91261   55   57  558.90091  281.58310  49.6%  17.3   61s
 627999 152675  322.01841   35   69  558.90091  287.64237  48.5%  17.0   65s
 679533 162929  490.74750   47   63  558.90091  293.80310  47.4%  16.8   70s
 710815 169554  458.66042   56   62  558.90091  296.99029  46.9%  16.6   75s
H711059 168808                     556.4510038  297.00144  46.6%  16.6   75s
H711195 165469                     546.4166155  297.00144  45.6%  16.6   75s
H711603 155780                     522.8999698  297.00144  43.2%  16.6   75s
 768158 165352  420.07820   42   62  522.89997  303.61806  41.9%  16.3   80s
 836009 177029  358.02437   47   72  522.89997  310.76615  40.6%  16.0   85s
 907611 189146  385.39603   38   73  522.89997  317.53393  39.3%  15.7   90s
 963725 198179  369.11495   51   59  522.89997  322.23134  38.4%  15.5   95s
 1008026 205287  390.88424   37   74  522.89997  326.01713  37.7%  15.3  100s
 1078410 216402  422.07836   67   44  522.89997  331.49423  36.6%  15.1  105s
 1097434 219095     cutoff   62       522.89997  332.82954  36.3%  15.0  110s
 1166253 228454     cutoff   86       522.89997  338.05564  35.3%  14.8  115s
 1237507 238011  354.68312   85   43  522.89997  343.20718  34.4%  14.6  120s
 1269010 242357     cutoff   53       522.89997  345.39364  33.9%  14.5  125s
 1338311 250817  515.80282   70   48  522.89997  349.97581  33.1%  14.3  130s
 1409658 259555  452.58780   42   56  522.89997  354.43348  32.2%  14.2  135s
 1460129 265465     cutoff   37       522.89997  357.46348  31.6%  14.1  140s
 1523311 272580     cutoff   56       522.89997  361.18696  30.9%  13.9  147s
 1558480 276377  434.15579   64   54  522.89997  363.20943  30.5%  13.9  150s
 1628085 283000  499.62342   46   56  522.89997  367.20573  29.8%  13.8  155s
 1699395 289745     cutoff   61       522.89997  371.07338  29.0%  13.6  160s
 1763875 295486  480.82269   71   49  522.89997  374.48230  28.4%  13.5  165s
 1782480 296962  438.09618   80   45  522.89997  375.37284  28.2%  13.5  170s
 1854741 302887     cutoff   52       522.89997  379.02335  27.5%  13.4  175s
 1924124 308236  403.13049   71   51  522.89997  382.34153  26.9%  13.3  180s
 1995975 313127  445.84482   46   64  522.89997  385.72272  26.2%  13.1  185s
 2066493 317522     cutoff   55       522.89997  388.93491  25.6%  13.0  190s
 2138542 321674     cutoff   83       522.89997  392.18495  25.0%  12.9  195s
 2189663 324454  409.42549   66   54  522.89997  394.43745  24.6%  12.9  200s
 2258980 327604  476.15133   36   69  522.89997  397.39630  24.0%  12.8  205s
 2324617 329726  415.15525   59   61  522.89997  400.23963  23.5%  12.7  211s
 2368378 330703     cutoff   39       522.89997  402.14204  23.1%  12.6  215s
 2441331 332268  455.98384   77   43  522.89997  405.15812  22.5%  12.6  220s
 2515320 333142     cutoff   70       522.89997  408.15335  21.9%  12.5  225s
 2577629 333852     cutoff   54       522.89997  410.67331  21.5%  12.4  231s
 2630939 333620  499.95936   57   53  522.89997  412.80155  21.1%  12.3  235s
 2664144 333248  451.91681   46   65  522.89997  414.16921  20.8%  12.3  240s
 2725970 332254     cutoff   61       522.89997  416.63990  20.3%  12.2  245s
 2800121 330318  478.99056   35   72  522.89997  419.68493  19.7%  12.1  250s
 2871130 328659     cutoff   59       522.89997  422.36788  19.2%  12.1  255s
 2944724 325463  500.14513   66   40  522.89997  425.19473  18.7%  12.0  260s
 3013776 322319  472.79130   81   49  522.89997  427.81236  18.2%  11.9  265s
 3079665 318539     cutoff   71       522.89997  430.35149  17.7%  11.8  274s
 3089489 317592     cutoff   35       522.89997  430.73118  17.6%  11.8  275s
 3163095 310938     cutoff   64       522.89997  433.70215  17.1%  11.7  280s
 3235438 304175  482.24514   79   36  522.89997  436.53120  16.5%  11.7  285s
 3309364 295020  444.55360   42   66  522.89997  439.59645  15.9%  11.6  290s
 3381386 285653     cutoff   59       522.89997  442.58423  15.4%  11.5  295s
 3457723 273971  521.92372   36   71  522.89997  445.86442  14.7%  11.4  300s
 3526684 262474     cutoff   80       522.89997  448.79006  14.2%  11.4  307s
 3565232 255657     cutoff   76       522.89997  450.53017  13.8%  11.3  310s
 3641573 241473     cutoff  100       522.89997  454.03934  13.2%  11.2  315s
 3713965 225778  473.22436   69   42  522.89997  457.52231  12.5%  11.2  320s
 3763354 214792     cutoff   68       522.89997  459.95124  12.0%  11.1  325s
 3831173 197777  513.48114   72   50  522.89997  463.51570  11.4%  11.0  330s
 3900556 178624  515.96079  104   16  522.89997  467.34592  10.6%  11.0  338s
 3928012 169930  482.06844   92   32  522.89997  468.99244  10.3%  10.9  340s
 4003104 144703  495.01446   89   35  522.89997  473.98083  9.36%  10.9  345s
 4077671 117061     cutoff   79       522.89997  479.86614  8.23%  10.8  350s
 4144254 88957     cutoff   39       522.89997  486.40576  6.98%  10.7  355s
 4212537 55084     cutoff   68       522.89997  496.01417  5.14%  10.6  360s

Explored 4217436 nodes (44716616 simplex iterations) in 360.30 seconds (387.83 work units)
Thread count was 16 (of 48 available processors)

Solution count 10: 522.9 546.417 556.451 ... 705.91

Optimal solution found (tolerance 5.00e-02)
Best objective 5.228999698142e+02, best bound 4.968743126289e+02, gap 4.9772%

Gurobi MIQP converged to an optimal solution.
Cluster Assignments from Gurobi MIQP using MSE
############################################################################

Ridge Regression using scikit-learn:
Optimal weights w* in X Space (including bias):
[ 1.07607299e+00  2.24460003e-03 -4.74534954e-04 -7.30062869e-05]

Mean Squared Error on training data (scikit-learn) with lambda = 0.1:
113.34510818564951
Cluster assignments:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2 2 2 2 1 2 2 1 2 2 1 1 2
 2 1 2 2 2 1 2 2 2 2 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 2 1 1 1 1 2 2 1]
-----------------------------------
Regression weights for cluster 0: y = -1.2918x_0 + -1.7636x_1 + -1.1814x_2 + -9.6752
Regression weights for cluster 0 after refit: y = -1.2877x_1 + -1.76x_2 + -1.1772x_3 + -9.691
-----------------------------------
Regression weights for cluster 1: y = 0.7x_0 + -0.0291x_1 + -0.6502x_2 + 10.6393
Regression weights for cluster 1 after refit: y = 0.7008x_1 + -0.0348x_2 + -0.6556x_3 + 10.6543
-----------------------------------
Regression weights for cluster 2: y = 4.115x_0 + 0.3389x_1 + 0.7651x_2 + -0.8278
Regression weights for cluster 2 after refit: y = 4.1168x_1 + 0.3397x_2 + 0.7659x_3 + -0.8321
{'time_milp': 361.0501437187195, 'time_greedy': np.float64(0.4763999700546265), 'time_refit_milp_assignment': 363.77188539505005, 'mse_refit_ground_truth_assignment': np.float64(2.737776417928324), 'r2_refit_ground_truth_assignment': 0.9775011926668062, 'weight_mismatch_refit_ground_truth_assignment': np.float64(2.9909774787860224), 'refit-weight_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'rand_score_refit_ground_truth_assignment': 1.0, 'label_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'mse_milp': np.float64(3.9986744571225423), 'r2_milp': 0.9671392427775238, 'weight_mismatch_milp': np.float64(4.85698205233668), 'refit-weight_mismatch_milp': np.float64(4.548494115935333), 'rand_score_milp': np.float64(0.8227699530516432), 'label_mismatch_milp': np.float64(0.16666666666666666), 'mse_refit_milp_assignment': np.float64(3.9986311629695686), 'r2_refit_milp_assignment': 0.9671395985650892, 'weight_mismatch_refit_milp_assignment': np.float64(4.857272107929646), 'refit-weight_mismatch_refit_milp_assignment': np.float64(4.52195745161864), 'rand_score_refit_milp_assignment': np.float64(0.8227699530516432), 'label_mismatch_refit_milp_assignment': np.float64(0.16666666666666666), 'mse_greedy': np.float64(9.141616328610565), 'r2_greedy': np.float64(0.9248749959476159), 'weight_mismatch_greedy': np.float64(15.327851275667086), 'refit-weight_mismatch_greedy': np.float64(14.929841011203402), 'rand_score_greedy': np.float64(0.7333724569640063), 'label_mismatch_greedy': np.float64(0.3319444444444445), 'mse_greedy_sem': np.float64(1.9305576976715075), 'r2_greedy_sem': np.float64(0.015865154437407568), 'weight_mismatch_greedy_sem': np.float64(2.476715908186776), 'refit-weight_mismatch_greedy_sem': np.float64(2.6122348738673367), 'rand_score_greedy_sem': np.float64(0.024240429878092228), 'label_mismatch_greedy_sem': np.float64(0.03653735171267122), 'mse_ground_truth': np.float64(2.823955770248749), 'r2_ground_truth': np.float64(0.9753841821603813), 'weight_mismatch_ground_truth': 0, 'rand_score_ground_truth': 1, 'label_mismatch_ground_truth': 0, 'mse_baseline_sklearn': np.float64(113.34510818564951), 'r2_baseline_sklearn': np.float64(0.06853980678282978), 'mse_milp_val': np.float64(7.41180389555527), 'r2_milp_val': 0.9287358380536673, 'label_mismatch_milp_val': np.float64(0.041666666666666664), 'mse_refit_milp_assignment_val': np.float64(7.420803404625572), 'r2_refit_milp_assignment_val': 0.9286493081776936, 'label_mismatch_refit_milp_assignment_val': np.float64(0.041666666666666664), 'mse_greedy_val': np.float64(24.21497815155244), 'label_mismatch_greedy_val': np.float64(0.275), 'mse_greedy_val_sem': np.float64(6.252534509996676), 'label_mismatch_greedy_val_sem': np.float64(0.04051590727961128), 'r2_greedy_val': np.float64(0.7671740714087176), 'r2_greedy_val_sem': np.float64(0.06011783881149962), 'mse_refit_ground_truth_assignment_val': np.float64(3.491321156902192), 'r2_refit_ground_truth_assignment_val': 0.9664311037045462, 'label_mismatch_refit_ground_truth_assignment_val': np.float64(0.041666666666666664), 'mse_ground_truth_val': np.float64(3.1776850855260306), 'r2_ground_truth_val': 0.9694467004604407, 'label_mismatch_ground_truth_val': np.float64(0.041666666666666664), 'mse_baseline_sklearn_val': np.float64(104.46778425727943), 'r2_baseline_sklearn_val': -0.00445305898471271}

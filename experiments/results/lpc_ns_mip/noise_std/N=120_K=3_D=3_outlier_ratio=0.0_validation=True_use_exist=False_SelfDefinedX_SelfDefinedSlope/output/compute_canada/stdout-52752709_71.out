==================== Evaluating with noise_std = 0.3 in Dataset 1 with random state = 7 ====================
ODS is enabled
mse 0.08964313758837524
Splitting data into training and validation set
Not whitening the data...
Set parameter TokenServer to value "license1.computecanada.ca"
Set parameter Threads to value 16
Read parameters from file gurobi.env
Set parameter MIPGap to value 0.05
Set parameter TimeLimit to value 36000
using MSE:
Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (linux64 - "AlmaLinux 9.3 (Shamrock Pampas Cat)")

CPU model: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 48 physical cores, 48 logical processors, using up to 16 threads

Optimize a model with 72 rows, 288 columns and 216 nonzeros
Model fingerprint: 0x4ef95c35
Model has 72 quadratic objective terms
Model has 216 general constraints
Variable types: 72 continuous, 216 integer (216 binary)
Coefficient statistics:
  Matrix range     [1e+00, 1e+00]
  Objective range  [0e+00, 0e+00]
  QObjective range [2e+00, 2e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 1e+00]
  GenCon rhs range [9e-01, 2e+01]
  GenCon coe range [2e-05, 1e+01]
Presolve added 216 rows and 216 columns
Presolve time: 0.04s
Presolved: 288 rows, 504 columns, 16200 nonzeros
Presolved model has 216 SOS constraint(s)
Presolved model has 72 quadratic objective terms
Variable types: 288 continuous, 216 integer (216 binary)
Found heuristic solution: objective 8682.0165737
Found heuristic solution: objective 7315.4253183

Root relaxation: objective 0.000000e+00, 701 iterations, 0.01 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0    0.00000    0   82 7315.42532    0.00000   100%     -    0s
H    0     0                    7094.5513015    0.00000   100%     -    0s
H    0     0                    4272.4900328    0.00000   100%     -    0s
     0     0    0.00000    0   84 4272.49003    0.00000   100%     -    0s
     0     2    0.00000    0   84 4272.49003    0.00000   100%     -    0s
H   33    48                    3729.9204139    0.00000   100%  61.8    0s
H   41    48                    3444.7171531    0.00000   100%  50.7    0s
H  129   160                    3224.7963014    0.00000   100%  43.5    0s
H  191   224                    2722.1853645    0.00000   100%  44.1    0s
H  223   240                    2371.9921568    0.00000   100%  43.6    0s
H  228   240                    2240.0613538    0.00000   100%  44.1    0s
H  232   240                    2141.0275840    0.00000   100%  43.9    0s
H  238   240                    1918.8291962    0.00000   100%  44.1    0s
H 1284  1296                    1854.8290045    0.00000   100%  33.0    1s
H 1288  1296                    1760.6980411    0.00000   100%  33.0    1s
H 1295  1312                    1673.4325561    0.00000   100%  32.9    2s
H 1301  1312                    1639.3318776    0.00000   100%  32.8    2s
H 1304  1270                    1057.8291148    0.00000   100%  32.7    2s
H 3012  2212                     717.2406755    0.00000   100%  32.1    4s
H 3239  2284                     601.3221533    0.00000   100%  33.2    4s
  3771  2554  449.43852   44   67  601.32215    0.00000   100%  33.5    5s
H 3777  2454                     521.5804637    0.00000   100%  33.5    5s
* 6018  2566             128     503.7503109    0.00000   100%  32.6    6s
* 6019  2401             128     469.0645876    0.00000   100%  32.6    6s
* 6020  2145             128     419.9445611    0.00000   100%  32.6    6s
H10801  3191                     404.6013756    1.46032   100%  31.3    8s
H10854  2955                     364.4806912    1.46032   100%  31.3    8s
*20360  6276             112     358.3057589    9.70898  97.3%  28.0    9s
 23328  7314  223.31076   23   77  358.30576   11.54159  96.8%  27.6   10s
*36423 11759             121     357.2375926   21.23322  94.1%  24.9   11s
*36426 10942             123     322.7407718   21.23322  93.4%  24.9   11s
*75618 21556             123     307.2713398   48.00523  84.4%  21.0   14s
 85152 24358  292.31545   58   55  307.27134   54.54093  82.2%  20.3   15s
H89689 25364                     306.5034249   55.58105  81.9%  19.9   15s
*151818 40156             128     279.0217049   80.68950  71.1%  17.1   19s
 154558 41058     cutoff   81       279.02170   81.78064  70.7%  17.0   20s
H189514 49371                     273.8273354   92.48382  66.2%  15.9   22s
H190212 46048                     261.6582487   92.48402  64.7%  15.9   22s
 225187 54611     cutoff   71       261.65825  101.57978  61.2%  15.0   25s
H253173 55059                     241.5185790  107.81394  55.4%  14.5   27s
 290597 63428  119.79402   36   78  241.51858  115.08584  52.3%  13.8   30s
H297932 59893                     230.9107837  116.18703  49.7%  13.7   33s
H303696 56940                     222.8806921  117.20216  47.4%  13.6   34s
 306959 57608  189.80358   72   51  222.88069  117.88660  47.1%  13.6   35s
H340789 63311                     222.6248701  123.41968  44.6%  13.0   37s
H342820 62709                     220.9461464  123.67478  44.0%  13.0   37s
 375897 68599  174.44191  102   21  220.94615  128.00419  42.1%  12.5   40s
 435219 77662     cutoff   74       220.94615  134.02649  39.3%  11.8   45s
H500157 87685                     220.7322860  138.90494  37.1%  11.2   49s
 501183 87679     cutoff   67       220.73229  138.91797  37.1%  11.2   50s
 569941 96729     cutoff   79       220.73229  143.78117  34.9%  10.6   55s
 630153 104201     cutoff   96       220.73229  147.32442  33.3%  10.3   60s
 701480 112212  196.03117   57   58  220.73229  151.03008  31.6%   9.9   65s
 760249 115079  167.49486   80   44  220.73229  154.50736  30.0%   9.6   70s
 807483 116138     cutoff   99       220.73229  157.17452  28.8%   9.5   75s
 870009 116506     cutoff   68       220.73229  160.47452  27.3%   9.2   80s
 932583 116272  199.52850   73   50  220.73229  163.75911  25.8%   9.1   85s
 1000540 113974     cutoff   34       220.73229  167.29412  24.2%   8.9   90s
 1052899 110939  195.60319   91   36  220.73229  170.02814  23.0%   8.8   95s
 1122860 105445  178.15807   48   64  220.73229  173.75719  21.3%   8.6  100s
 1180193 98930  190.33312   66   56  220.73229  177.04505  19.8%   8.5  106s
 1236110 91022     cutoff  104       220.73229  180.43710  18.3%   8.4  110s
 1294693 80366     cutoff   71       220.73229  184.36000  16.5%   8.3  115s
 1355070 65150     cutoff   72       220.73229  189.23037  14.3%   8.2  120s
 1420931 41918  203.48886   75   39  220.73229  196.62952  10.9%   8.1  125s

Explored 1480041 nodes (11846119 simplex iterations) in 129.57 seconds (136.72 work units)
Thread count was 16 (of 48 available processors)

Solution count 10: 220.732 220.946 222.625 ... 306.503

Optimal solution found (tolerance 5.00e-02)
Best objective 2.207322859968e+02, best bound 2.099460578768e+02, gap 4.8866%

Gurobi MIQP converged to an optimal solution.
Cluster Assignments from Gurobi MIQP using MSE
############################################################################

Ridge Regression using scikit-learn:
Optimal weights w* in X Space (including bias):
[ 4.22888721e-01  1.91540785e-04 -5.81802869e-04  3.40385420e-04]

Mean Squared Error on training data (scikit-learn) with lambda = 0.1:
116.22670839344076
Cluster assignments:  [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 1 1 1 2 1 1 1 2 1 1
 1 1 1 1 2 1 1 1 2 1 1 1 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 2 2]
-----------------------------------
Regression weights for cluster 0: y = -1.4447x_0 + -1.7341x_1 + -1.0296x_2 + -9.9049
Regression weights for cluster 0 after refit: y = -1.4396x_1 + -1.7308x_2 + -1.0242x_3 + -9.9239
-----------------------------------
Regression weights for cluster 1: y = 1.5037x_0 + -2.1331x_1 + 3.8809x_2 + 2.1772
Regression weights for cluster 1 after refit: y = 1.5037x_1 + -2.135x_2 + 3.8815x_3 + 2.1793
-----------------------------------
Regression weights for cluster 2: y = -0.1419x_0 + 1.488x_1 + -0.6289x_2 + 8.4376
Regression weights for cluster 2 after refit: y = -0.1476x_1 + 1.4812x_2 + -0.6317x_3 + 8.4567
{'time_milp': 130.04977917671204, 'time_greedy': np.float64(0.4715552568435669), 'time_refit_milp_assignment': 132.7586007118225, 'mse_refit_ground_truth_assignment': np.float64(0.062320905518811545), 'r2_refit_ground_truth_assignment': 0.9994679739863227, 'weight_mismatch_refit_ground_truth_assignment': np.float64(0.3312427499230328), 'refit-weight_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'rand_score_refit_ground_truth_assignment': 1.0, 'label_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'mse_milp': np.float64(2.614524911871359), 'r2_milp': 0.9776801178522175, 'weight_mismatch_milp': np.float64(7.209659977344634), 'refit-weight_mismatch_milp': np.float64(7.051396629876366), 'rand_score_milp': np.float64(0.8125978090766823), 'label_mismatch_milp': np.float64(0.18055555555555555), 'mse_refit_milp_assignment': np.float64(2.614478470869101), 'r2_refit_milp_assignment': 0.9776805143134226, 'weight_mismatch_refit_milp_assignment': np.float64(7.181825487773224), 'refit-weight_mismatch_refit_milp_assignment': np.float64(7.033365892653797), 'rand_score_refit_milp_assignment': np.float64(0.8125978090766823), 'label_mismatch_refit_milp_assignment': np.float64(0.18055555555555555), 'mse_greedy': np.float64(5.543659924591376), 'r2_greedy': np.float64(0.9526744474216156), 'weight_mismatch_greedy': np.float64(12.540333766408486), 'refit-weight_mismatch_greedy': np.float64(12.373131146355519), 'rand_score_greedy': np.float64(0.8179186228482003), 'label_mismatch_greedy': np.float64(0.21388888888888888), 'mse_greedy_sem': np.float64(1.8381325939496052), 'r2_greedy_sem': np.float64(0.015691915071326553), 'weight_mismatch_greedy_sem': np.float64(3.4620337493005926), 'refit-weight_mismatch_greedy_sem': np.float64(3.4836259073650706), 'rand_score_greedy_sem': np.float64(0.0340315274789341), 'label_mismatch_greedy_sem': np.float64(0.04340197367676194), 'mse_ground_truth': np.float64(0.08964313758837524), 'r2_ground_truth': np.float64(0.999200816771906), 'weight_mismatch_ground_truth': 0, 'rand_score_ground_truth': 1, 'label_mismatch_ground_truth': 0, 'mse_baseline_sklearn': np.float64(116.22670839344076), 'r2_baseline_sklearn': np.float64(0.007786683543400841), 'mse_milp_val': np.float64(6.316277133256676), 'r2_milp_val': 0.9395318733634456, 'label_mismatch_milp_val': np.float64(0.14583333333333334), 'mse_refit_milp_assignment_val': np.float64(6.318541010965177), 'r2_refit_milp_assignment_val': 0.9395102003999136, 'label_mismatch_refit_milp_assignment_val': np.float64(0.14583333333333334), 'mse_greedy_val': np.float64(13.599715459466916), 'label_mismatch_greedy_val': np.float64(0.2125), 'mse_greedy_val_sem': np.float64(4.7172390268018285), 'label_mismatch_greedy_val_sem': np.float64(0.04257775824390034), 'r2_greedy_val': np.float64(0.8698047442702773), 'r2_greedy_val_sem': np.float64(0.045159925828092716), 'mse_refit_ground_truth_assignment_val': np.float64(0.12342013080284649), 'r2_refit_ground_truth_assignment_val': 0.9988184520815921, 'label_mismatch_refit_ground_truth_assignment_val': np.float64(0.0), 'mse_ground_truth_val': np.float64(0.11720636304349293), 'r2_ground_truth_val': 0.9988779388469502, 'label_mismatch_ground_truth_val': np.float64(0.0), 'mse_baseline_sklearn_val': np.float64(104.8869182939203), 'r2_baseline_sklearn_val': -0.004122416434347631}

==================== Evaluating with noise_std = 2.3 in Dataset 1 with random state = 8 ====================
ODS is enabled
mse 6.647320589384458
Splitting data into training and validation set
Not whitening the data...
Set parameter TokenServer to value "license1.computecanada.ca"
Set parameter Threads to value 16
Read parameters from file gurobi.env
Set parameter MIPGap to value 0.05
Set parameter TimeLimit to value 36000
using MSE:
Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (linux64 - "AlmaLinux 9.3 (Shamrock Pampas Cat)")

CPU model: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 48 physical cores, 48 logical processors, using up to 16 threads

Optimize a model with 72 rows, 288 columns and 216 nonzeros
Model fingerprint: 0xf1e74409
Model has 72 quadratic objective terms
Model has 216 general constraints
Variable types: 72 continuous, 216 integer (216 binary)
Coefficient statistics:
  Matrix range     [1e+00, 1e+00]
  Objective range  [0e+00, 0e+00]
  QObjective range [2e+00, 2e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 1e+00]
  GenCon rhs range [3e-01, 2e+01]
  GenCon coe range [2e-04, 1e+01]
Presolve added 216 rows and 216 columns
Presolve time: 0.03s
Presolved: 288 rows, 504 columns, 16200 nonzeros
Presolved model has 216 SOS constraint(s)
Presolved model has 72 quadratic objective terms
Variable types: 288 continuous, 216 integer (216 binary)
Found heuristic solution: objective 8370.0423989
Found heuristic solution: objective 7012.9412894

Root relaxation: objective 0.000000e+00, 667 iterations, 0.01 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0    0.00000    0   82 7012.94129    0.00000   100%     -    0s
H    0     0                    5848.5333938    0.00000   100%     -    0s
H    0     0                    3666.6985984    0.00000   100%     -    0s
     0     0    0.00000    0   84 3666.69860    0.00000   100%     -    0s
     0     2    0.00000    0   84 3666.69860    0.00000   100%     -    0s
H   33    48                    3184.4636751    0.00000   100%  41.3    0s
H   44    48                    3034.7565284    0.00000   100%  33.4    0s
H  192   216                    2343.9132814    0.00000   100%  50.2    0s
H  203   216                    2327.3536833    0.00000   100%  50.4    0s
H  210   216                    2059.5006541    0.00000   100%  51.5    0s
H  266   280                    1663.3881571    0.00000   100%  51.5    0s
H  267   280                    1170.5755345    0.00000   100%  51.3    0s
H  270   280                    1159.6383400    0.00000   100%  51.3    0s
H 1287  1187                    1076.0528619    0.00000   100%  34.1    1s
H 1293  1183                    1064.9843372    0.00000   100%  34.0    1s
H 1303  1165                    1021.1222265    0.00000   100%  33.8    1s
H 2954  2154                     984.6519006    0.00000   100%  31.7    4s
H 2973  2064                     953.7607732    0.00000   100%  31.9    4s
  3037  2106  110.20864   22   74  953.76077    0.00000   100%  32.5    5s
H 3048  2008                     920.9793554    0.00000   100%  32.4    5s
H 7863  2780                     827.2970908    8.13673  99.0%  33.0    9s
H 7875  2410                     696.0998677    8.13673  98.8%  33.1    9s
 11169  3917     cutoff   29       696.09987   47.82087  93.1%  32.7   10s
*22605  7720             119     620.8206784   87.46550  85.9%  30.8   11s
*26880  9302             117     602.8226229   94.72636  84.3%  29.4   11s
H30787  9698                     573.6486747  102.56797  82.1%  28.9   12s
*44203 11673             121     490.8592014  126.21215  74.3%  26.3   13s
 66421 19413  200.25216   45   67  490.85920  145.22731  70.4%  22.5   15s
*85150 20455             121     419.2978296  157.04876  62.5%  20.5   16s
 136331 34173  254.34234   44   66  419.29783  179.97330  57.1%  17.2   20s
 207822 53868  291.73968   76   44  419.29783  197.54959  52.9%  14.8   25s
 266486 69891  273.64056   80   40  419.29783  207.31787  50.6%  13.5   31s
H276416 70497                     406.3629672  208.87778  48.6%  13.3   32s
 318037 81538  298.13205   54   55  406.36297  214.81509  47.1%  12.7   35s
 394506 100775     cutoff   73       406.36297  222.35157  45.3%  11.8   40s
 468727 119470  234.79478   61   50  406.36297  228.04926  43.9%  11.2   45s
 547001 139323  361.80795   99   22  406.36297  232.75076  42.7%  10.6   50s
 618191 157726     cutoff   72       406.36297  236.37808  41.8%  10.3   55s
 694823 176911  256.89588   67   49  406.36297  239.83114  41.0%  10.0   60s
 771110 195812     cutoff   75       406.36297  242.72252  40.3%   9.7   65s
 844177 214596  263.94804   72   39  406.36297  245.10562  39.7%   9.5   70s
 920399 233843     cutoff   73       406.36297  247.18737  39.2%   9.3   75s
 996936 252747  345.29471   56   56  406.36297  249.00921  38.7%   9.1   80s
 1070694 270660     cutoff   78       406.36297  250.66713  38.3%   9.0   85s
 1147244 287531  281.54895   76   42  406.36297  252.42065  37.9%   8.8   90s
 1221302 303712  307.47953   61   54  406.36297  254.01396  37.5%   8.7   95s
 1299459 316750  369.82268   66   55  406.36297  255.95719  37.0%   8.7  100s
 1373884 326110     cutoff   67       406.36297  257.88968  36.5%   8.6  105s
 1427007 331620  393.82550   74   39  406.36297  259.27688  36.2%   8.5  110s
 1504005 338664  352.17783   86   35  406.36297  261.25050  35.7%   8.5  115s
 1574016 344096  331.21592   68   45  406.36297  263.05789  35.3%   8.5  120s
 1599460 345975     cutoff   81       406.36297  263.70032  35.1%   8.4  125s
 1673707 349919     cutoff   83       406.36297  265.71400  34.6%   8.4  130s
 1750352 352988     cutoff   79       406.36297  267.91935  34.1%   8.4  135s
 1821858 355268  300.16016   82   39  406.36297  270.02041  33.6%   8.3  140s
 1881258 357175  324.63396   70   45  406.36297  271.79411  33.1%   8.3  145s
 1942461 358558  383.59561   87   26  406.36297  273.63723  32.7%   8.3  150s
 2002845 359128     cutoff   79       406.36297  275.50972  32.2%   8.3  155s
 2047324 359818  307.11941   66   49  406.36297  276.85904  31.9%   8.3  160s
 2123138 360485     cutoff   77       406.36297  279.21100  31.3%   8.2  165s
 2171411 360315     cutoff   88       406.36297  280.66616  30.9%   8.2  170s
 2245217 359866     cutoff   74       406.36297  283.03842  30.3%   8.2  175s
 2282909 359385  372.95569   76   34  406.36297  284.28598  30.0%   8.2  180s
 2354354 358214  380.65328   80   40  406.36297  286.64832  29.5%   8.2  185s
 2405367 357312  351.58551   69   57  406.36297  288.24128  29.1%   8.1  190s
 2477690 355130     cutoff   65       406.36297  290.67394  28.5%   8.1  195s
 2533938 352910  295.89740   78   48  406.36297  292.66262  28.0%   8.1  200s
 2600037 349757     cutoff   68       406.36297  294.95204  27.4%   8.1  205s
 2649430 347503     cutoff   63       406.36297  296.59534  27.0%   8.1  210s
 2712946 343558     cutoff   81       406.36297  298.87677  26.5%   8.0  215s
 2790028 338012     cutoff   81       406.36297  301.74399  25.7%   8.0  220s
 2844132 333639  386.53931   81   29  406.36297  303.77119  25.2%   8.0  225s
 2916963 327430  389.60724   70   40  406.36297  306.65598  24.5%   8.0  230s
 2962122 322891  404.99115   71   40  406.36297  308.52930  24.1%   7.9  235s
 3037661 315709  380.39964   96   25  406.36297  311.61498  23.3%   7.9  240s
 3083591 310858  318.82450   67   54  406.36297  313.51768  22.8%   7.9  246s
 3142303 304623  365.32849   89   46  406.36297  315.91449  22.3%   7.9  250s
 3222695 295540     cutoff   83       406.36297  319.34668  21.4%   7.9  255s
 3286361 287561  352.41005   55   59  406.36297  322.09131  20.7%   7.8  260s
 3364192 277531     cutoff   74       406.36297  325.44491  19.9%   7.8  265s
 3434604 267749     cutoff   72       406.36297  328.54859  19.1%   7.8  270s
 3497703 258851     cutoff   67       406.36297  331.31549  18.5%   7.7  275s
 3535913 253300     cutoff   63       406.36297  333.00938  18.1%   7.7  280s
 3615004 241734     cutoff   85       406.36297  336.48863  17.2%   7.7  285s
 3653409 235751     cutoff   88       406.36297  338.25753  16.8%   7.7  290s
 3729561 223508  399.13280   65   55  406.36297  341.71031  15.9%   7.6  295s
 3790401 213588     cutoff   75       406.36297  344.47539  15.2%   7.6  300s
 3857124 201926  349.02416   78   37  406.36297  347.56180  14.5%   7.6  305s
 3919457 190324  355.21029   61   51  406.36297  350.50570  13.7%   7.5  310s
 3984565 177805  372.93340   84   37  406.36297  353.55160  13.0%   7.5  315s
 4049324 164641     cutoff   73       406.36297  356.73204  12.2%   7.5  320s
 4112802 151127     cutoff   71       406.36297  359.92285  11.4%   7.4  325s
 4145805 143739     cutoff   85       406.36297  361.67678  11.0%   7.4  331s
 4203143 130108  406.23762   34   61  406.36297  364.88868  10.2%   7.4  335s
 4281794 109025  397.96868   67   37  406.36297  369.86662  8.98%   7.4  340s
 4321215 97564  393.61838   96   24  406.36297  372.60746  8.31%   7.3  345s
 4396270 72598  394.02496   92   20  406.36297  378.47460  6.86%   7.3  350s
 4456743 49161     cutoff   71       406.36297  384.46552  5.39%   7.2  357s

Explored 4471879 nodes (32375511 simplex iterations) in 358.49 seconds (379.31 work units)
Thread count was 16 (of 48 available processors)

Solution count 10: 406.363 419.298 490.859 ... 953.761

Optimal solution found (tolerance 5.00e-02)
Best objective 4.063629671761e+02, best bound 3.861997652923e+02, gap 4.9619%

Gurobi MIQP converged to an optimal solution.
Cluster Assignments from Gurobi MIQP using MSE
############################################################################

Ridge Regression using scikit-learn:
Optimal weights w* in X Space (including bias):
[ 6.91240120e-01 -2.66225684e-05 -5.47402507e-05  2.90789539e-04]

Mean Squared Error on training data (scikit-learn) with lambda = 0.1:
113.74930417076953
Cluster assignments:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 1 2 2 2 1 2 2 2
 1 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 2 1 2 1 1 2 1]
-----------------------------------
Regression weights for cluster 0: y = -1.8786x_0 + -1.6058x_1 + -0.0432x_2 + -10.1162
Regression weights for cluster 0 after refit: y = -1.8753x_1 + -1.6023x_2 + -0.0376x_3 + -10.132
-----------------------------------
Regression weights for cluster 1: y = -0.6468x_0 + 1.9048x_1 + 0.3637x_2 + 9.1357
Regression weights for cluster 1 after refit: y = -0.6508x_1 + 1.8998x_2 + 0.3606x_3 + 9.1505
-----------------------------------
Regression weights for cluster 2: y = 2.3018x_0 + 1.3576x_1 + 0.6003x_2 + 0.8991
Regression weights for cluster 2 after refit: y = 2.3024x_1 + 1.3581x_2 + 0.6003x_3 + 0.8984
{'time_milp': 359.16973185539246, 'time_greedy': np.float64(0.4920817732810974), 'time_refit_milp_assignment': 362.0258700847626, 'mse_refit_ground_truth_assignment': np.float64(5.247363622371719), 'r2_refit_ground_truth_assignment': 0.953928808971967, 'weight_mismatch_refit_ground_truth_assignment': np.float64(3.8646294734760085), 'refit-weight_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'rand_score_refit_ground_truth_assignment': 1.0, 'label_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'mse_milp': np.float64(4.997774730937686), 'r2_milp': 0.9561201679711239, 'weight_mismatch_milp': np.float64(4.1427008863987504), 'refit-weight_mismatch_milp': np.float64(1.0877782540965173), 'rand_score_milp': np.float64(0.8513302034428795), 'label_mismatch_milp': np.float64(0.1388888888888889), 'mse_refit_milp_assignment': np.float64(4.997736258346581), 'r2_refit_milp_assignment': 0.9561205057556232, 'weight_mismatch_refit_milp_assignment': np.float64(4.139301282330802), 'refit-weight_mismatch_refit_milp_assignment': np.float64(1.055843710934438), 'rand_score_refit_milp_assignment': np.float64(0.8513302034428795), 'label_mismatch_refit_milp_assignment': np.float64(0.1388888888888889), 'mse_greedy': np.float64(6.844775106086014), 'r2_greedy': np.float64(0.9399037375431811), 'weight_mismatch_greedy': np.float64(15.2777857935485), 'refit-weight_mismatch_greedy': np.float64(13.603706335001629), 'rand_score_greedy': np.float64(0.7660602503912363), 'label_mismatch_greedy': np.float64(0.2826388888888889), 'mse_greedy_sem': np.float64(0.5407054327305043), 'r2_greedy_sem': np.float64(0.004747325528388479), 'weight_mismatch_greedy_sem': np.float64(2.25046163999088), 'refit-weight_mismatch_greedy_sem': np.float64(2.398568057892956), 'rand_score_greedy_sem': np.float64(0.01701508610201519), 'label_mismatch_greedy_sem': np.float64(0.028199917367009653), 'mse_ground_truth': np.float64(6.647320589384458), 'r2_ground_truth': np.float64(0.943683528029697), 'weight_mismatch_ground_truth': 0, 'rand_score_ground_truth': 1, 'label_mismatch_ground_truth': 0, 'mse_baseline_sklearn': np.float64(113.74930417076953), 'r2_baseline_sklearn': np.float64(0.0012954506498055185), 'mse_milp_val': np.float64(8.070728959109966), 'r2_milp_val': 0.9350126956060117, 'label_mismatch_milp_val': np.float64(0.1875), 'mse_refit_milp_assignment_val': np.float64(8.076691203338775), 'r2_refit_milp_assignment_val': 0.9349646862895631, 'label_mismatch_refit_milp_assignment_val': np.float64(0.1875), 'mse_greedy_val': np.float64(19.59220934881037), 'label_mismatch_greedy_val': np.float64(0.30520833333333336), 'mse_greedy_val_sem': np.float64(4.755353674150929), 'label_mismatch_greedy_val_sem': np.float64(0.024523743939555098), 'r2_greedy_val': np.float64(0.8422391732949122), 'r2_greedy_val_sem': np.float64(0.03829116530722861), 'mse_refit_ground_truth_assignment_val': np.float64(9.299035053668781), 'r2_refit_ground_truth_assignment_val': 0.9251221017748336, 'label_mismatch_refit_ground_truth_assignment_val': np.float64(0.20833333333333334), 'mse_ground_truth_val': np.float64(6.387399627003713), 'r2_ground_truth_val': 0.9485672377366128, 'label_mismatch_ground_truth_val': np.float64(0.20833333333333334), 'mse_baseline_sklearn_val': np.float64(124.27819488136966), 'r2_baseline_sklearn_val': -0.000715662886247248}

==================== Evaluating with noise_std = 2.9 in Dataset 1 with random state = 8 ====================
ODS is enabled
mse 10.567857496545049
Splitting data into training and validation set
Not whitening the data...
Set parameter TokenServer to value "license1.computecanada.ca"
Set parameter Threads to value 16
Read parameters from file gurobi.env
Set parameter MIPGap to value 0.05
Set parameter TimeLimit to value 36000
using MSE:
Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (linux64 - "AlmaLinux 9.3 (Shamrock Pampas Cat)")

CPU model: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 48 physical cores, 48 logical processors, using up to 16 threads

Optimize a model with 72 rows, 288 columns and 216 nonzeros
Model fingerprint: 0x55855aff
Model has 72 quadratic objective terms
Model has 216 general constraints
Variable types: 72 continuous, 216 integer (216 binary)
Coefficient statistics:
  Matrix range     [1e+00, 1e+00]
  Objective range  [0e+00, 0e+00]
  QObjective range [2e+00, 2e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 1e+00]
  GenCon rhs range [1e-01, 2e+01]
  GenCon coe range [9e-05, 1e+01]
Presolve added 216 rows and 216 columns
Presolve time: 0.04s
Presolved: 288 rows, 504 columns, 16200 nonzeros
Presolved model has 216 SOS constraint(s)
Presolved model has 72 quadratic objective terms
Variable types: 288 continuous, 216 integer (216 binary)
Found heuristic solution: objective 8650.4573518
Found heuristic solution: objective 7583.9998333

Root relaxation: objective 0.000000e+00, 659 iterations, 0.02 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0    0.00000    0   82 7583.99983    0.00000   100%     -    0s
H    0     0                    5831.6282004    0.00000   100%     -    0s
H    0     0                    5745.8094056    0.00000   100%     -    0s
H    0     0                    3702.3893891    0.00000   100%     -    0s
     0     0    7.85101    0   82 3702.38939    7.85101   100%     -    0s
H    0     0                    3634.3120797    7.85101   100%     -    0s
     0     2    7.85101    0   82 3634.31208    7.85101   100%     -    0s
H   33    48                    3512.9598181    7.85101   100%  36.0    0s
H   86    96                    3315.1724533    7.85101   100%  33.7    0s
H   87    96                    3238.9607520    7.85101   100%  34.2    0s
H   94    96                    2846.3163273    7.85101   100%  37.3    0s
H  190   228                    2764.6801850    7.85101   100%  41.8    0s
H  207   228                    2050.7746572    7.85101   100%  44.3    0s
H  217   228                    1999.6121801    7.85101   100%  43.9    0s
H  625   641                    1630.9119039    7.85101   100%  41.2    0s
H  632   641                    1557.9563123    7.85101  99.5%  41.1    0s
H  633   641                    1285.6259665    7.85101  99.4%  41.2    0s
H  653   657                    1029.3923822    7.85101  99.2%  40.7    1s
H 3187  2064                    1019.1104822    7.85101  99.2%  33.9    4s
  3860  2442     cutoff   39      1019.11048    7.85101  99.2%  33.7    5s
*26146  9234             117     811.6373783   90.93775  88.8%  25.0    7s
 55366 18155  697.37470   28   73  811.63738  158.81100  80.4%  23.5   10s
*65942 18060             112     724.9100013  179.43326  75.2%  22.9   10s
*66386 15005             111     661.6959161  180.23014  72.8%  22.8   10s
H67266 14862                     658.3449643  183.03633  72.2%  22.8   12s
H67430 12325                     601.5853997  183.17316  69.6%  22.8   12s
 92785 18677     cutoff   41       601.58540  233.21740  61.2%  21.0   15s
 165575 39683     cutoff   39       601.58540  292.91729  51.3%  16.8   20s
H195086 47810                     601.3908424  306.70248  49.0%  15.7   24s
 205967 51392  352.51341   55   53  601.39084  310.67380  48.3%  15.3   25s
 251055 63683     cutoff   38       601.39084  324.07736  46.1%  14.1   30s
H265522 66365                     592.1830927  327.29958  44.7%  13.8   32s
 305060 76361  459.99097   65   57  592.18309  336.05879  43.3%  13.1   35s
 375145 93918  373.88444   51   52  592.18309  348.07385  41.2%  12.1   40s
H375568 93245                     586.1326118  348.07385  40.6%  12.1   41s
 439447 107945  557.55399   51   57  586.13261  356.64543  39.2%  11.4   45s
 502794 120732     cutoff   70       586.13261  364.88580  37.7%  10.9   50s
 567371 133947     cutoff   52       586.13261  371.59575  36.6%  10.5   55s
H567710 133556                     583.5241531  371.62272  36.3%  10.5   55s
 616746 143032  524.62999   62   50  583.52415  376.65637  35.5%  10.2   60s
 695424 157673  437.98997   52   60  583.52415  384.71168  34.1%   9.9   65s
 774637 171654  443.11295   69   41  583.52415  392.11220  32.8%   9.6   70s
 856102 185281  534.71591   49   70  583.52415  398.89846  31.6%   9.4   75s
 909731 193511     cutoff   64       583.52415  402.72130  31.0%   9.2   80s
H909735 193365                     583.0062244  402.72130  30.9%   9.2   80s
 974256 203238     cutoff   70       583.00622  407.30479  30.1%   9.1   85s
 1034283 212078  520.88972   52   64  583.00622  411.32558  29.4%   9.0   90s
 1062358 215969     cutoff   51       583.00622  413.15744  29.1%   8.9   97s
 1108130 221914  539.87425   72   52  583.00622  416.08626  28.6%   8.9  100s
 1187763 232268  494.75550   59   57  583.00622  420.70597  27.8%   8.8  105s
 1267274 241395     cutoff   75       583.00622  425.52598  27.0%   8.6  110s
 1346569 250772     cutoff   50       583.00622  429.68176  26.3%   8.6  115s
 1403951 257486     cutoff   61       583.00622  432.54413  25.8%   8.5  120s
 1483574 266174  526.49443   71   48  583.00622  436.36468  25.2%   8.4  125s
 1526359 270836     cutoff   60       583.00622  438.41453  24.8%   8.4  130s
 1603977 278900     cutoff   67       583.00622  441.83639  24.2%   8.3  135s
 1660338 284301  536.99755   68   47  583.00622  444.22805  23.8%   8.3  140s
 1704013 288509  465.65170   60   56  583.00622  446.10369  23.5%   8.2  145s
 1769049 294405  485.81524   54   68  583.00622  448.84759  23.0%   8.2  150s
 1839350 300697  526.49248   72   39  583.00622  451.57479  22.5%   8.2  158s
 1871370 302557  469.10542   59   50  583.00622  452.89639  22.3%   8.1  160s
 1949030 307709     cutoff   69       583.00622  455.83978  21.8%   8.1  165s
 2026100 311369     cutoff   72       583.00622  458.83120  21.3%   8.1  170s
 2083388 314257  521.97124   49   58  583.00622  461.05823  20.9%   8.0  175s
 2122119 315668     cutoff   68       583.00622  462.53377  20.7%   8.0  183s
 2146819 316852  478.00696   72   43  583.00622  463.42120  20.5%   8.0  185s
 2225027 319217  530.10870   73   40  583.00622  466.32756  20.0%   7.9  190s
 2302743 321081     cutoff   57       583.00622  469.22158  19.5%   7.9  195s
 2380934 322370     cutoff   51       583.00622  471.95635  19.0%   7.9  200s
 2441886 322027     cutoff   68       583.00622  474.30848  18.6%   7.8  205s
H2491590 318897                     579.9192142  476.26614  17.9%   7.8  209s
H2491769 318104                     578.4531743  476.26614  17.7%   7.8  209s
 2504065 317374     cutoff   66       578.45317  476.80318  17.6%   7.8  210s
 2559414 314171  523.44169   49   66  578.45317  479.12688  17.2%   7.8  217s
 2595702 311924     cutoff   80       578.45317  480.64284  16.9%   7.8  220s
 2672522 306014     cutoff   46       578.45317  483.86981  16.4%   7.7  225s
 2749805 299606     cutoff   42       578.45317  487.08062  15.8%   7.7  230s
 2810441 293518     cutoff   82       578.45317  489.70299  15.3%   7.7  236s
 2865146 287755     cutoff   43       578.45317  491.92898  15.0%   7.7  240s
 2940533 279038  544.39834   64   55  578.45317  495.15574  14.4%   7.6  249s
H2941453 277420                     575.9526345  495.17601  14.0%   7.6  249s
 2944770 276537     cutoff   64       575.95263  495.36889  14.0%   7.6  250s
 3022419 265278     cutoff   87       575.95263  498.94144  13.4%   7.6  255s
 3098106 253821     cutoff   78       575.95263  502.33803  12.8%   7.6  260s
 3174462 241590     cutoff   61       575.95263  505.90051  12.2%   7.6  265s
 3207905 235834  557.00110   55   55  575.95263  507.48335  11.9%   7.5  273s
 3229120 231618     cutoff   60       575.95263  508.56567  11.7%   7.5  275s
*3260360 224072             108     574.4045542  510.16416  11.2%   7.5  277s
 3303147 214220     cutoff   88       574.40455  512.48055  10.8%   7.5  280s
 3379018 195942     cutoff   64       574.40455  516.71768  10.0%   7.5  285s
 3456647 175344     cutoff   72       574.40455  521.46372  9.22%   7.4  290s
 3519449 157114     cutoff   75       574.40455  525.48330  8.52%   7.4  295s
 3596772 133620     cutoff   71       574.40455  530.99553  7.56%   7.4  300s
 3653543 115545  550.08217   81   28  574.40455  535.21439  6.82%   7.3  305s
 3702143 99134     cutoff   91       574.40455  539.06806  6.15%   7.3  312s
 3744351 83931     cutoff   76       574.40455  542.85094  5.49%   7.3  315s

Explored 3775528 nodes (27403029 simplex iterations) in 316.96 seconds (324.31 work units)
Thread count was 16 (of 48 available processors)

Solution count 10: 574.405 575.953 578.453 ... 601.585

Optimal solution found (tolerance 5.00e-02)
Best objective 5.744045542081e+02, best bound 5.458418377742e+02, gap 4.9726%

Gurobi MIQP converged to an optimal solution.
Cluster Assignments from Gurobi MIQP using MSE
############################################################################

Ridge Regression using scikit-learn:
Optimal weights w* in X Space (including bias):
[ 7.40871949e-01 -3.02452846e-05 -3.70150468e-05  3.32647407e-04]

Mean Squared Error on training data (scikit-learn) with lambda = 0.1:
117.1742776811995
Cluster assignments:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 1 1 1 2 2 1 1 2 2 2
 2 2 2 2 2 2 2 2 1 2 1 2 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 2 2 1 2 1 1 2 1]
-----------------------------------
Regression weights for cluster 0: y = -2.0188x_0 + -1.5901x_1 + 0.2236x_2 + -10.1504
Regression weights for cluster 0 after refit: y = -2.0156x_1 + -1.5866x_2 + 0.2294x_3 + -10.1664
-----------------------------------
Regression weights for cluster 1: y = -0.8627x_0 + 2.1307x_1 + 0.4262x_2 + 9.4723
Regression weights for cluster 1 after refit: y = -0.8677x_1 + 2.1248x_2 + 0.4233x_3 + 9.489
-----------------------------------
Regression weights for cluster 2: y = 2.2552x_0 + 1.0901x_1 + 0.7461x_2 + 0.7658
Regression weights for cluster 2 after refit: y = 2.2558x_1 + 1.0905x_2 + 0.7462x_3 + 0.7652
{'time_milp': 317.6337070465088, 'time_greedy': np.float64(0.4777966022491455), 'time_refit_milp_assignment': 320.32376742362976, 'mse_refit_ground_truth_assignment': np.float64(8.342217025358444), 'r2_refit_ground_truth_assignment': 0.9289226096432104, 'weight_mismatch_refit_ground_truth_assignment': np.float64(4.872793683948911), 'refit-weight_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'rand_score_refit_ground_truth_assignment': 1.0, 'label_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'mse_milp': np.float64(7.271681343139749), 'r2_milp': 0.9380437919793485, 'weight_mismatch_milp': np.float64(4.663423298588503), 'refit-weight_mismatch_milp': np.float64(1.0781422130500085), 'rand_score_milp': np.float64(0.8137715179968701), 'label_mismatch_milp': np.float64(0.19444444444444445), 'mse_refit_milp_assignment': np.float64(7.271639504257536), 'r2_refit_milp_assignment': 0.9380441484551576, 'weight_mismatch_refit_milp_assignment': np.float64(4.662028596336365), 'refit-weight_mismatch_refit_milp_assignment': np.float64(1.0517315972336374), 'rand_score_refit_milp_assignment': np.float64(0.8137715179968701), 'label_mismatch_refit_milp_assignment': np.float64(0.19444444444444445), 'mse_greedy': np.float64(10.91813487320282), 'r2_greedy': np.float64(0.9069752642502884), 'weight_mismatch_greedy': np.float64(20.222360331781463), 'refit-weight_mismatch_greedy': np.float64(18.34309220564653), 'rand_score_greedy': np.float64(0.7355242566510173), 'label_mismatch_greedy': np.float64(0.3076388888888889), 'mse_greedy_sem': np.float64(1.2175123742141511), 'r2_greedy_sem': np.float64(0.01037345372617211), 'weight_mismatch_greedy_sem': np.float64(2.8581191827634598), 'refit-weight_mismatch_greedy_sem': np.float64(3.0206668511170234), 'rand_score_greedy_sem': np.float64(0.02025244899717495), 'label_mismatch_greedy_sem': np.float64(0.028432971090430196), 'mse_ground_truth': np.float64(10.567857496545049), 'r2_ground_truth': np.float64(0.913337702466464), 'weight_mismatch_ground_truth': 0, 'rand_score_ground_truth': 1, 'label_mismatch_ground_truth': 0, 'mse_baseline_sklearn': np.float64(117.1742776811995), 'r2_baseline_sklearn': np.float64(0.0016512577885581248), 'mse_milp_val': np.float64(12.181104989918216), 'r2_milp_val': 0.9053998968676165, 'label_mismatch_milp_val': np.float64(0.16666666666666666), 'mse_refit_milp_assignment_val': np.float64(12.188260080127959), 'r2_refit_milp_assignment_val': 0.9053443294726778, 'label_mismatch_refit_milp_assignment_val': np.float64(0.16666666666666666), 'mse_greedy_val': np.float64(35.57126491995231), 'label_mismatch_greedy_val': np.float64(0.3364583333333333), 'mse_greedy_val_sem': np.float64(6.641454325552034), 'label_mismatch_greedy_val_sem': np.float64(0.02771591704686204), 'r2_greedy_val': np.float64(0.7237487623034256), 'r2_greedy_val_sem': np.float64(0.051578429433638405), 'mse_refit_ground_truth_assignment_val': np.float64(14.951073529892787), 'r2_refit_ground_truth_assignment_val': 0.8838879478472336, 'label_mismatch_refit_ground_truth_assignment_val': np.float64(0.20833333333333334), 'mse_ground_truth_val': np.float64(10.573311885920186), 'r2_ground_truth_val': 0.9178862348131175, 'label_mismatch_ground_truth_val': np.float64(0.20833333333333334), 'mse_baseline_sklearn_val': np.float64(128.83292696450476), 'r2_baseline_sklearn_val': -0.0005338750282637417}

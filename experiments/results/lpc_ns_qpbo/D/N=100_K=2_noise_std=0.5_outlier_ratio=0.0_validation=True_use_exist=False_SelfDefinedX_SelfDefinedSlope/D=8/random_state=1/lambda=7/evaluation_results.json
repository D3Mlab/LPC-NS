{"time_milp": 0.3768749237060547, "time_greedy": 1.2031495690345764, "time_refit_milp_assignment": 0.9863359928131104, "mse_refit_ground_truth_assignment": 0.2151900119437072, "r2_refit_ground_truth_assignment": 0.9972873337609062, "weight_mismatch_refit_ground_truth_assignment": 0.5865492430335947, "refit-weight_mismatch_refit_ground_truth_assignment": 0.0, "rand_score_refit_ground_truth_assignment": 1.0, "label_mismatch_refit_ground_truth_assignment": 0.0, "mse_milp": 0.388458963042436, "r2_milp": 0.9951031207034169, "weight_mismatch_milp": 0.7001888982211901, "refit-weight_mismatch_milp": 0.3375699127688139, "rand_score_milp": 0.844632768361582, "label_mismatch_milp": 0.08333333333333333, "mse_refit_milp_assignment": 0.3883963305939059, "r2_refit_milp_assignment": 0.995103910242518, "weight_mismatch_refit_milp_assignment": 0.6744148326757551, "refit-weight_mismatch_refit_milp_assignment": 0.3327283066370853, "rand_score_refit_milp_assignment": 0.844632768361582, "label_mismatch_refit_milp_assignment": 0.08333333333333333, "mse_greedy": 10.95643808502086, "r2_greedy": 0.8618841115091674, "weight_mismatch_greedy": 13.058559466145592, "refit-weight_mismatch_greedy": 12.589587799600086, "rand_score_greedy": 0.6891525423728813, "label_mismatch_greedy": 0.2633333333333333, "mse_greedy_sem": 2.0131376150869373, "r2_greedy_sem": 0.025377434546194354, "weight_mismatch_greedy_sem": 2.432246466089868, "refit-weight_mismatch_greedy_sem": 2.4471898144054505, "rand_score_greedy_sem": 0.05310335235309015, "label_mismatch_greedy_sem": 0.046541184679217494, "mse_ground_truth": 0.25949395931279357, "r2_ground_truth": 0.9966313420654032, "weight_mismatch_ground_truth": 0, "rand_score_ground_truth": 1, "label_mismatch_ground_truth": 0, "mse_baseline_sklearn": 57.512289667372286, "r2_baseline_sklearn": 0.2750051682023331, "mse_milp_val": 25.391722670298666, "r2_milp_val": 0.6524028777779849, "label_mismatch_milp_val": 0.25, "mse_refit_milp_assignment_val": 25.422157684318442, "r2_refit_milp_assignment_val": 0.6519862410879332, "label_mismatch_refit_milp_assignment_val": 0.25, "mse_greedy_val": 76.11749733678437, "label_mismatch_greedy_val": 0.3425, "mse_greedy_val_sem": 10.602348479260959, "label_mismatch_greedy_val_sem": 0.026100766272276373, "r2_greedy_val": -0.0420018904805143, "r2_greedy_val_sem": 0.1451396531081725, "mse_refit_ground_truth_assignment_val": 24.728127793424846, "r2_refit_ground_truth_assignment_val": 0.6614870849630462, "label_mismatch_refit_ground_truth_assignment_val": 0.2, "mse_ground_truth_val": 24.44192653975019, "r2_ground_truth_val": 0.6654050047294751, "label_mismatch_ground_truth_val": 0.2, "mse_baseline_sklearn_val": 73.88628795983075, "r2_baseline_sklearn_val": -0.01145800149061782, "random_state": 7}
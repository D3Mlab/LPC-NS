==================== Evaluating with noise_std = 2.6 in Dataset 1 with random state = 12 ====================
ODS is enabled
mse 7.850080665910758
Splitting data into training and validation set
Not whitening the data...
Set parameter TokenServer to value "license1.computecanada.ca"
Set parameter Threads to value 16
Read parameters from file gurobi.env
Set parameter MIPGap to value 0.05
Set parameter TimeLimit to value 18000
using QPBO:
Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (linux64 - "AlmaLinux 9.3 (Shamrock Pampas Cat)")

CPU model: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 48 physical cores, 48 logical processors, using up to 16 threads

Optimize a model with 72 rows, 216 columns and 216 nonzeros
Model fingerprint: 0x78d4f873
Model has 7884 quadratic objective terms
Variable types: 0 continuous, 216 integer (216 binary)
Coefficient statistics:
  Matrix range     [1e+00, 1e+00]
  Objective range  [0e+00, 0e+00]
  QObjective range [7e-03, 4e+02]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 1e+00]
Found heuristic solution: objective 1348.5383095
Presolve time: 0.06s
Presolved: 72 rows, 216 columns, 216 nonzeros
Presolved model has 7884 quadratic objective terms
Variable types: 0 continuous, 216 integer (216 binary)
Found heuristic solution: objective 5125.5957041

Root relaxation: objective 8.984344e+03, 301 iterations, 0.03 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0 8984.34404    0  213 5125.59570 8984.34404  75.3%     -    0s
H    0     0                    6419.6541089 8984.34404  40.0%     -    0s
     0     0 8882.18749    0  207 6419.65411 8882.18749  38.4%     -    0s
     0     2 8836.78670    0  207 6419.65411 8836.78670  37.7%     -    0s
 29955 18402 7196.34788   26  177 6419.65411 7548.75831  17.6%   5.2    5s
 74258 44607 6870.81760   35  142 6419.65411 7391.80476  15.1%   5.1   10s
 114495 69287 7167.03508   30  149 6419.65411 7340.58097  14.3%   5.1   15s
 171183 97351 6725.96583   36  164 6419.65411 7265.57641  13.2%   5.0   20s
 222973 123945 6892.69211   28  154 6419.65411 7228.93301  12.6%   5.0   25s
 275547 149399 6601.94365   34  138 6419.65411 7198.36849  12.1%   4.9   30s
 327311 174643 6923.91435   25  180 6419.65411 7173.26776  11.7%   4.9   35s
 374905 197904 6841.42869   34  142 6419.65411 7152.90043  11.4%   4.9   40s
 428931 222175 6540.69672   38  131 6419.65411 7133.73220  11.1%   4.9   45s
 482505 247841 6510.43654   47  104 6419.65411 7116.90098  10.9%   4.8   50s
 537245 272034 6848.85152   32  168 6419.65411 7102.26015  10.6%   4.8   55s
 590262 294636 6426.02128   42  138 6419.65411 7089.99883  10.4%   4.8   60s
 643242 317522     cutoff   48      6419.65411 7077.88058  10.3%   4.8   65s
 691815 338424 7058.72461   27  149 6419.65411 7067.68523  10.1%   4.8   70s
 744778 360711     cutoff   44      6419.65411 7057.70065  9.94%   4.8   75s
 800146 384381 6689.91399   35  139 6419.65411 7047.42183  9.78%   4.8   80s
 853410 407168     cutoff   40      6419.65411 7038.57055  9.64%   4.8   85s
 908672 430482 6503.40513   43  120 6419.65411 7029.99786  9.51%   4.8   90s
 955632 450004     cutoff   43      6419.65411 7023.33564  9.40%   4.8   95s
 1004231 470549 6773.83103   32  170 6419.65411 7016.55992  9.30%   4.8  100s
 1061219 491914 6567.29109   40  118 6419.65411 7009.42490  9.19%   4.7  105s
 1115474 513162 6496.26713   34  157 6419.65411 7002.83959  9.08%   4.7  110s
 1171647 535187 6611.93683   32  168 6419.65411 6996.51762  8.99%   4.7  115s
 1226226 556546     cutoff   41      6419.65411 6990.37006  8.89%   4.7  120s
 1274098 575335 6465.24227   36  145 6419.65411 6985.19790  8.81%   4.7  125s
 1329100 596601     cutoff   47      6419.65411 6979.48950  8.72%   4.7  130s
 1383523 616159 6604.35691   33  153 6419.65411 6974.34026  8.64%   4.7  135s
 1438642 635710     cutoff   39      6419.65411 6969.31252  8.56%   4.7  140s
 1493295 657009     cutoff   34      6419.65411 6964.28026  8.48%   4.7  145s
 1550116 678562 6619.92594   42  126 6419.65411 6959.15402  8.40%   4.7  150s
 1598816 696023 6444.08504   36  146 6419.65411 6955.10511  8.34%   4.7  155s
 1655589 716816 6847.07774   33  155 6419.65411 6950.59807  8.27%   4.7  160s
 1708694 734807     cutoff   36      6419.65411 6946.53297  8.21%   4.7  165s
 1764591 753949 6506.28502   40  133 6419.65411 6942.51972  8.14%   4.7  170s
 1805333 773262 6466.75487   42  123 6419.65411 6939.57634  8.10%   4.7  175s
 1869982 790436 6455.06627   35  160 6419.65411 6935.10924  8.03%   4.7  180s
 1920742 808175 6481.18431   38  139 6419.65411 6931.33772  7.97%   4.7  185s
 1976546 827126 6615.41460   38  146 6419.65411 6927.81990  7.92%   4.7  190s
 2031338 845438     cutoff   43      6419.65411 6924.30556  7.86%   4.7  195s
 2080822 861926 6436.47752   39  129 6419.65411 6921.11469  7.81%   4.7  200s
 2136105 880259 6894.45974   33  158 6419.65411 6917.58514  7.76%   4.7  205s
 2193055 898882 6423.98642   41  120 6419.65411 6914.02738  7.70%   4.7  210s
 2249078 916828 6561.29798   36  156 6419.65411 6910.74293  7.65%   4.6  215s
 2303374 934912 6476.53426   39  121 6419.65411 6907.68358  7.60%   4.6  220s
 2354693 951200     cutoff   45      6419.65411 6904.91257  7.56%   4.6  225s
 2409044 968675 6766.45675   32  168 6419.65411 6901.92786  7.51%   4.6  230s
 2466208 987490     cutoff   43      6419.65411 6898.89574  7.47%   4.6  235s
 2521196 1005134 6614.60863   37  142 6419.65411 6896.02231  7.42%   4.6  240s
 2579081 1022998 6550.17065   43  131 6419.65411 6893.10947  7.38%   4.6  245s
 2628305 1038475 6529.84476   42  133 6419.65411 6890.69322  7.34%   4.6  250s
 2680377 1054166 6477.46476   38  127 6419.65411 6888.19006  7.30%   4.6  255s
 2736801 1071571 6551.09083   31  169 6419.65411 6885.49365  7.26%   4.6  260s
 2791331 1088274 6440.26164   33  163 6419.65411 6883.02143  7.22%   4.6  265s
 2849720 1106265 6698.65786   37  148 6419.65411 6880.33402  7.18%   4.6  270s
 2902590 1121654 6599.64232   37  145 6419.65411 6877.97523  7.14%   4.6  275s
 2954448 1138036 6667.19414   42  152 6419.65411 6875.64096  7.10%   4.6  280s
 3012208 1155044     cutoff   35      6419.65411 6873.12790  7.06%   4.6  285s
 3068259 1171618 6470.16631   39  139 6419.65411 6870.81014  7.03%   4.6  290s
 3120691 1187475 6420.64559   34  166 6419.65411 6868.61525  6.99%   4.6  295s
 3177796 1203444 6505.11689   37  135 6419.65411 6866.29268  6.96%   4.6  300s
 3227863 1219144 6442.12307   37  142 6419.65411 6864.33920  6.93%   4.6  305s
 3281177 1234656 6517.03573   39  115 6419.65411 6862.22534  6.89%   4.6  310s
 3336145 1250373 6857.98024   28  163 6419.65411 6860.08699  6.86%   4.6  315s
 3390581 1265309     cutoff   33      6419.65411 6858.09300  6.83%   4.6  320s
 3447959 1281685     cutoff   45      6419.65411 6855.94493  6.80%   4.6  325s
 3497978 1296210 6655.95461   37  127 6419.65411 6854.06145  6.77%   4.6  330s
 3555592 1312209 6778.93095   41  134 6419.65411 6851.94621  6.73%   4.6  335s
 3610015 1327447 6831.14478   37  155 6419.65411 6850.06879  6.70%   4.6  340s
 3667656 1343933     cutoff   45      6419.65411 6848.01717  6.67%   4.6  345s
 3720335 1358966 6659.03499   46  114 6419.65411 6846.16861  6.64%   4.6  350s
 3776097 1374140 6461.83888   40  142 6419.65411 6844.30470  6.61%   4.6  355s
 3831540 1388945 6518.05545   40  118 6419.65411 6842.38169  6.58%   4.6  360s
 3889107 1404871 6569.07288   41  154 6419.65411 6840.44408  6.55%   4.6  365s
 3948312 1421478 6430.68627   41  118 6419.65411 6838.49842  6.52%   4.6  370s
 3993811 1433599 6762.82535   41  135 6419.65411 6837.06360  6.50%   4.6  375s
 4052303 1448886 6491.15522   37  132 6419.65411 6835.20860  6.47%   4.6  380s
 4108864 1462908 6444.70420   34  157 6419.65411 6833.47509  6.45%   4.6  385s
 4165547 1477484 6513.18555   37  150 6419.65411 6831.78149  6.42%   4.6  390s
 4223165 1492646 6432.62252   39  139 6419.65411 6830.04537  6.39%   4.6  395s
 4270655 1505341     cutoff   38      6419.65411 6828.60501  6.37%   4.6  400s
 4327340 1519675 6493.34642   38  145 6419.65411 6826.95211  6.34%   4.6  405s
 4385013 1534887 6662.79995   36  135 6419.65411 6825.19132  6.32%   4.6  410s
 4439850 1548066     cutoff   40      6419.65411 6823.64678  6.29%   4.6  415s
 4495163 1562179 6778.84979   30  172 6419.65411 6822.07758  6.27%   4.6  420s
 4546971 1575446 6440.81244   40  127 6419.65411 6820.66032  6.25%   4.6  425s
 4598578 1587677 6453.93676   41  134 6419.65411 6819.22918  6.22%   4.6  430s
 4655165 1602061 6749.85538   35  145 6419.65411 6817.68268  6.20%   4.6  435s
 4712577 1615622 6437.05750   36  156 6419.65411 6816.13415  6.18%   4.6  440s
 4769441 1630458 6554.19729   34  153 6419.65411 6814.59973  6.15%   4.5  445s
 4817103 1642073 6725.48839   34  161 6419.65411 6813.32486  6.13%   4.5  450s
 4875302 1655962 6433.59927   34  154 6419.65411 6811.78927  6.11%   4.5  455s
 4933787 1670115 6445.24273   41  133 6419.65411 6810.22238  6.08%   4.5  460s
 4991228 1684160 6594.08918   34  148 6419.65411 6808.78424  6.06%   4.5  465s
 5049219 1697582 6513.19802   35  143 6419.65411 6807.28196  6.04%   4.5  470s
 5097947 1708853 6721.63906   43  112 6419.65411 6806.01376  6.02%   4.5  475s
 5154970 1721798 6656.85977   39  141 6419.65411 6804.60666  6.00%   4.5  480s
 5212053 1735065 6485.87551   44  136 6419.65411 6803.23906  5.98%   4.5  485s
 5269844 1748300 6475.20143   40  146 6419.65411 6801.84437  5.95%   4.5  490s
 5327846 1761013 6500.94190   39  136 6419.65411 6800.45807  5.93%   4.5  495s
 5372036 1771660 6527.74986   36  146 6419.65411 6799.41690  5.92%   4.5  500s
 5429030 1785098 6676.05255   32  158 6419.65411 6798.08216  5.89%   4.5  505s
 5489147 1798329 6590.72536   44  117 6419.65411 6796.72777  5.87%   4.5  510s
 5546116 1810785 6479.48581   29  167 6419.65411 6795.44310  5.85%   4.5  515s
 5603362 1823991 6665.46647   36  151 6419.65411 6794.14332  5.83%   4.5  520s
 5654377 1837587 6519.16628   43  135 6419.65411 6792.99260  5.82%   4.5  526s
 5713669 1848124 6599.19314   42  130 6419.65411 6791.80245  5.80%   4.5  530s
 5768813 1860095 6577.89710   36  147 6419.65411 6790.62807  5.78%   4.5  535s
 5827230 1872569 6451.68117   38  132 6419.65411 6789.37892  5.76%   4.5  540s
 5885388 1885102     cutoff   43      6419.65411 6788.11470  5.74%   4.5  545s
 5943000 1897353 6467.92509   39  149 6419.65411 6786.93386  5.72%   4.5  550s
 5994187 1908142 6605.73291   37  138 6419.65411 6785.79375  5.70%   4.5  555s
 6047647 1919610 6649.79812   42  121 6419.65411 6784.66116  5.69%   4.5  560s
 6104714 1931612 6422.51134   40  155 6419.65411 6783.45676  5.67%   4.5  565s
 6161950 1943461     cutoff   37      6419.65411 6782.28574  5.65%   4.5  570s
 6215217 1954215 6479.87756   35  163 6419.65411 6781.21016  5.63%   4.5  575s
 6273069 1966347     cutoff   38      6419.65411 6780.04527  5.61%   4.5  580s
 6330218 1978028 6626.62740   39  131 6419.65411 6778.88361  5.60%   4.5  585s
 6387635 1990174     cutoff   36      6419.65411 6777.71425  5.58%   4.5  590s
 6447306 2002974 6540.56739   46  135 6419.65411 6776.52976  5.56%   4.5  595s
 6502702 2014758 6537.37043   30  163 6419.65411 6775.47115  5.54%   4.5  600s
 6554369 2025218 6436.49657   40  139 6419.65411 6774.47191  5.53%   4.5  605s
 6612050 2037180 6496.21341   37  143 6419.65411 6773.35760  5.51%   4.5  610s
 6672095 2048948 6427.22692   38  149 6419.65411 6772.20776  5.49%   4.5  615s
 6728615 2060126 6610.30193   38  145 6419.65411 6771.12136  5.47%   4.5  620s
 6782120 2070546 6478.82327   36  155 6419.65411 6770.15075  5.46%   4.5  625s
 6838669 2081181 6674.44505   37  143 6419.65411 6769.09532  5.44%   4.5  630s
 6898646 2092670 6709.26603   37  149 6419.65411 6767.99057  5.43%   4.5  635s
 6952585 2102846 6442.47447   40  130 6419.65411 6766.99858  5.41%   4.5  640s
 7007479 2113670 6638.31525   39  127 6419.65411 6765.99479  5.40%   4.5  645s
 7061547 2124116 6481.59107   41  126 6419.65411 6765.02539  5.38%   4.5  650s
 7118393 2135255 6425.99981   48  143 6419.65411 6763.98184  5.36%   4.5  655s
 7178626 2146381     cutoff   42      6419.65411 6762.89068  5.35%   4.5  660s
 7233918 2156605 6467.93062   34  153 6419.65411 6761.93765  5.33%   4.5  665s
 7286880 2166663 6707.28649   35  154 6419.65411 6761.02184  5.32%   4.5  670s
 7343938 2177521     cutoff   35      6419.65411 6760.01106  5.30%   4.5  675s
 7401438 2188181 6466.12048   38  148 6419.65411 6758.98649  5.29%   4.5  680s
 7460159 2198743 6442.82735   42  127 6419.65411 6757.96836  5.27%   4.5  685s
 7517803 2209432 6536.51415   38  135 6419.65411 6756.94536  5.25%   4.5  690s
 7562206 2217035 6699.91757   29  146 6419.65411 6756.18267  5.24%   4.5  695s
 7620303 2227714 6447.37229   47  132 6419.65411 6755.20321  5.23%   4.5  700s
 7679192 2238524 6515.28357   37  147 6419.65411 6754.15826  5.21%   4.5  705s
 7736836 2248974     cutoff   38      6419.65411 6753.19512  5.20%   4.5  710s
 7794308 2259192 6630.50578   34  156 6419.65411 6752.25343  5.18%   4.5  715s
 7846866 2268452 6503.92811   40  153 6419.65411 6751.40098  5.17%   4.5  720s
 7906273 2279279 6444.49417   38  126 6419.65411 6750.40476  5.15%   4.5  725s
 7964181 2289093 6505.95281   39  139 6419.65411 6749.48650  5.14%   4.5  730s
 8021707 2298987 6710.00482   40  139 6419.65411 6748.54988  5.12%   4.5  735s
 8076443 2307743     cutoff   44      6419.65411 6747.66978  5.11%   4.5  740s
 8131242 2317499     cutoff   34      6419.65411 6746.79752  5.10%   4.5  745s
 8189005 2327246 6745.68513   37  129 6419.65411 6745.88428  5.08%   4.5  750s
 8246370 2336997     cutoff   38      6419.65411 6744.96409  5.07%   4.5  755s
 8306243 2346805 6484.00962   46  124 6419.65411 6744.04037  5.05%   4.5  760s
 8365056 2356734 6621.10322   41  144 6419.65411 6743.13368  5.04%   4.5  765s
 8415981 2365229 6742.17754   36  124 6419.65411 6742.30836  5.03%   4.5  770s
 8473722 2374310 6459.93996   40  141 6419.65411 6741.41536  5.01%   4.5  775s

Explored 8525338 nodes (38147130 simplex iterations) in 779.74 seconds (698.08 work units)
Thread count was 16 (of 48 available processors)

Solution count 4: 6419.65 6144.83 5125.6 1348.54 

Optimal solution found (tolerance 5.00e-02)
Best objective 6.419654108945e+03, best bound 6.740614485516e+03, gap 4.9997%

Gurobi MIQP converged to an optimal solution.
Cluster Assignments from Gurobi MIQP using QPBO
############################################################################

Ridge Regression using scikit-learn:
Optimal weights w* in X Space (including bias):
[ 0.23860666 -0.0007166   0.00053746  0.00116695]

Mean Squared Error on training data (scikit-learn) with lambda = 0.1:
115.66272402748642
Cluster assignments:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
-----------------------------------
Regression weights for cluster 0: y = 0.8254x_0 + 1.1523x_1 + -0.1428x_2 + 6.0627
Regression weights for cluster 0 after refit: y = -0.1457x_1 + 0.923x_2 + 0.1242x_3 + 7.2614
-----------------------------------
Regression weights for cluster 1: y = -2.1765x_0 + -2.3478x_1 + -1.6322x_2 + -8.4658
Regression weights for cluster 1 after refit: y = -2.5889x_1 + -2.3186x_2 + -0.4976x_3 + -9.078
-----------------------------------
Regression weights for cluster 2: y = 0.0x_0 + 0.0x_1 + 0.0x_2 + 0.0
Regression weights for cluster 2 after refit: y = 0.0x_1 + 0.0x_2 + 0.0x_3 + 0.0
{'time_milp': 784.9143357276917, 'time_greedy': np.float64(0.5698903799057007), 'time_refit_milp_assignment': 792.4800021648407, 'mse_refit_ground_truth_assignment': np.float64(7.033087913451428), 'r2_refit_ground_truth_assignment': 0.9418085994220314, 'weight_mismatch_refit_ground_truth_assignment': np.float64(3.918925677035725), 'refit-weight_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'rand_score_refit_ground_truth_assignment': 1.0, 'label_mismatch_refit_ground_truth_assignment': np.float64(0.0), 'mse_milp': np.float64(11.653007908871805), 'r2_milp': 0.903583623650366, 'weight_mismatch_milp': np.float64(6.109511910209468), 'refit-weight_mismatch_milp': np.float64(4.756666182378379), 'rand_score_milp': np.float64(0.7656494522691706), 'label_mismatch_milp': np.float64(0.3333333333333333), 'mse_refit_milp_assignment': np.float64(12.977622467150635), 'r2_refit_milp_assignment': 0.8926238322584815, 'weight_mismatch_refit_milp_assignment': np.float64(4.648777341763772), 'refit-weight_mismatch_refit_milp_assignment': np.float64(2.640349849153655), 'rand_score_refit_milp_assignment': np.float64(0.7656494522691706), 'label_mismatch_refit_milp_assignment': np.float64(0.3333333333333333), 'mse_greedy': np.float64(120.9186918344978), 'r2_greedy': np.float64(-0.0004749152143226354), 'weight_mismatch_greedy': np.float64(22.956050966705313), 'refit-weight_mismatch_greedy': np.float64(21.847984545281225), 'rand_score_greedy': np.float64(0.8853677621283256), 'label_mismatch_greedy': np.float64(0.09722222222222224), 'mse_greedy_sem': np.float64(9.780580990248002e-15), 'r2_greedy_sem': np.float64(0.0), 'weight_mismatch_greedy_sem': np.float64(1.39989162600147e-15), 'refit-weight_mismatch_greedy_sem': np.float64(6.81918429234964e-16), 'rand_score_greedy_sem': np.float64(2.54702629954375e-17), 'label_mismatch_greedy_sem': np.float64(3.1837828744296875e-18), 'mse_ground_truth': np.float64(7.850080665910758), 'r2_ground_truth': np.float64(0.9349794813994586), 'weight_mismatch_ground_truth': 0, 'rand_score_ground_truth': 1, 'label_mismatch_ground_truth': 0, 'mse_baseline_sklearn': np.float64(115.66272402748642), 'r2_baseline_sklearn': np.float64(0.0430126867958468), 'mse_milp_val': np.float64(8.509947077827507), 'r2_milp_val': 0.9293908306783152, 'label_mismatch_milp_val': np.float64(0.3333333333333333), 'mse_refit_milp_assignment_val': np.float64(9.575994662652796), 'r2_refit_milp_assignment_val': 0.9205455659858914, 'label_mismatch_refit_milp_assignment_val': np.float64(0.3333333333333333), 'mse_greedy_val': np.float64(120.68871146816934), 'label_mismatch_greedy_val': np.float64(0.27083333333333326), 'mse_greedy_val_sem': np.float64(3.260193663416e-15), 'label_mismatch_greedy_val_sem': np.float64(1.273513149771875e-17), 'r2_greedy_val': np.float64(-0.0013845662419129656), 'r2_greedy_val_sem': np.float64(0.0), 'mse_refit_ground_truth_assignment_val': np.float64(8.601471316905812), 'r2_refit_ground_truth_assignment_val': 0.9286314310680689, 'label_mismatch_refit_ground_truth_assignment_val': np.float64(0.22916666666666666), 'mse_ground_truth_val': np.float64(6.025863223309988), 'r2_ground_truth_val': 0.9500018986307693, 'label_mismatch_ground_truth_val': np.float64(0.22916666666666666), 'mse_baseline_sklearn_val': np.float64(120.55751399334359), 'r2_baseline_sklearn_val': -0.00029598782540496416}

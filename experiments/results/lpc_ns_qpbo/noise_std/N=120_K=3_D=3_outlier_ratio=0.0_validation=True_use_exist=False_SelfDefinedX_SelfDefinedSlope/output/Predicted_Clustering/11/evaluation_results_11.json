{"time_milp": 3291.0705716609955, "time_greedy": 0.5724459409713745, "time_refit_milp_assignment": 3298.6573815345764, "mse_refit_ground_truth_assignment": 0.09363578582997467, "r2_refit_ground_truth_assignment": 0.999157515687916, "weight_mismatch_refit_ground_truth_assignment": 0.45218373196566153, "refit-weight_mismatch_refit_ground_truth_assignment": 0.0, "rand_score_refit_ground_truth_assignment": 1.0, "label_mismatch_refit_ground_truth_assignment": 0.0, "mse_milp": 5.667493592499091, "r2_milp": 0.9490069485913499, "weight_mismatch_milp": 4.7236050418544755, "refit-weight_mismatch_milp": 4.665440584508996, "rand_score_milp": 0.7746478873239436, "label_mismatch_milp": 0.3333333333333333, "mse_refit_milp_assignment": 6.510923722323704, "r2_refit_milp_assignment": 0.9414182190643023, "weight_mismatch_refit_milp_assignment": 6.320155445356867, "refit-weight_mismatch_refit_milp_assignment": 6.136887187315541, "rand_score_refit_milp_assignment": 0.7746478873239436, "label_mismatch_refit_milp_assignment": 0.3333333333333333, "mse_greedy": 111.4929602957042, "r2_greedy": -0.0031535392622217273, "weight_mismatch_greedy": 22.956050966705313, "refit-weight_mismatch_greedy": 22.795679544169282, "rand_score_greedy": 0.9311424100156496, "label_mismatch_greedy": 0.05555555555555556, "mse_greedy_sem": 3.260193663416e-15, "r2_greedy_sem": 0.0, "weight_mismatch_greedy_sem": 1.4350408191326673e-15, "refit-weight_mismatch_greedy_sem": 0.0, "rand_score_greedy_sem": 2.54702629954375e-17, "label_mismatch_greedy_sem": 1.5918914372148438e-18, "mse_ground_truth": 0.10451290827395981, "r2_ground_truth": 0.9990643520256753, "weight_mismatch_ground_truth": 0, "rand_score_ground_truth": 1, "label_mismatch_ground_truth": 0, "mse_baseline_sklearn": 106.78975678998275, "r2_baseline_sklearn": 0.03916334989492021, "mse_milp_val": 4.17948544169183, "r2_milp_val": 0.9628598624641211, "label_mismatch_milp_val": 0.3333333333333333, "mse_refit_milp_assignment_val": 5.802189358363648, "r2_refit_milp_assignment_val": 0.948440037946009, "label_mismatch_refit_milp_assignment_val": 0.3333333333333333, "mse_greedy_val": 113.0140456395759, "label_mismatch_greedy_val": 0.125, "mse_greedy_val_sem": 0.0, "label_mismatch_greedy_val_sem": 0.0, "r2_greedy_val": -0.0042760662998921894, "r2_greedy_val_sem": 0.0, "mse_refit_ground_truth_assignment_val": 0.156829717886059, "r2_refit_ground_truth_assignment_val": 0.9986063649764398, "label_mismatch_refit_ground_truth_assignment_val": 0.0, "mse_ground_truth_val": 0.10563745548623926, "r2_ground_truth_val": 0.9990612744845186, "label_mismatch_ground_truth_val": 0.0, "mse_baseline_sklearn_val": 112.54815583843633, "r2_baseline_sklearn_val": -0.0001360324291477255}